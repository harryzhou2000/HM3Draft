%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{physics}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    }
\usepackage{subcaption}


\journal{Journal's Name}

\begin{document}

\begin{frontmatter}

    %% Title, authors and addresses

    %% use the tnoteref command within \title for footnotes;
    %% use the tnotetext command for theassociated footnote;
    %% use the fnref command within \author or \address for footnotes;
    %% use the fntext command for theassociated footnote;
    %% use the corref command within \author for corresponding author footnotes;
    %% use the cortext command for theassociated footnote;
    %% use the ead command for the email address,
    %% and the form \ead[url] for the home page:
    \title{High order Accurate Implicit Time Marching Scheme for
        Solving Compressible Navier-Stokes Equations Based on Temporal
        Reconstruction}
    % \tnotetext[label1]{}
    % \ead{email address}
    % \ead[url]{home page}
    % \fntext[label2]{}
    % \cortext[cor1]{}
    % \fntext[label3]{}



    %% use optional labels to link authors explicitly to addresses:
    %% \author[label1,label2]{}
    %% \affiliation[label1]{organization={},
    %%             addressline={},
    %%             city={},
    %%             postcode={},
    %%             state={},
    %%             country={}}
    %%
    %% \affiliation[label2]{organization={},
    %%             addressline={},
    %%             city={},
    %%             postcode={},
    %%             state={},
    %%             country={}}

    \author[THUDEM]{Hanyu Zhou}
    \author[THUDEM]{Yuxin Ren}

    \affiliation[THUDEM]
    {
        organization=
            { Department of Engineering Mechanices, Tsinghua Universiy},%Department and Organization
        addressline={},
        city={},
        postcode={},
        state={Beijing},
        country={China}}



    \begin{abstract}
        Text of abstract

    \end{abstract}

    % %%Graphical abstract
    % \begin{graphicalabstract}
    % %\includegraphics{grabs}
    % \end{graphicalabstract}

    %%Research highlights
    \begin{highlights}
        \item Research highlight 1
        \item Research highlight 2
    \end{highlights}

    \begin{keyword}
        %% keywords here, in the form: keyword \sep keyword

        %% PACS codes here, in the form: \PACS code \sep code

        %% MSC codes here, in the form: \MSC code \sep code
        %% or \MSC[2008] code \sep code (2000 is the default)

    \end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:intro}

In computational fluid dynamics (CFD),
high-order numerical methods have the capability of
resolving complex flows effectively and efficiently,
which have been attracting great attention recently.
Popular high-order CFD methods, including discontinuous
Galerkin (DG) methods
\cite{reed1973triangularDG,
    BASSI1997251DG,
    BASSI1997267DG,
    cockburn1989DGII,
    cockburn2001rungeDG},
spectral volume
\cite{WANG2002210_SV}
and spectral difference
\cite{LIU2006780_SD} methods,
PnPm procedures
\cite{DUMBSER20088209_PNPM},
FR/CPR methods
\cite{huynh2007flux_FR,
    huynh2009reconstruction_FR,
    vincent2011new_FR,
    wang2009unifying_CPR},
and finite volume (FV) methods
\cite{wang2016compact_VR,
    wang2016compact1_VR,
    wang2017compact_VR,
    pan2018high_VR,
    zhang2019compact_VR,
    barth1990higher_FV,
    delanaye1999quadratic_FV,
    ollivier1997quasi_ENO,
    friedrich1998weighted_WENO,
    hu1999weighted_WENO,
    dumbser2007quadrature_WENO},
generally provides spacial discretization methods
which produce semi-discretized forms of the original
partial differential equations (PDEs).
The semi-discretized PDEs are
sets of first order ordinary
differential equations (ODEs),
which are usually further solved with
ODE integrators such as the
popular strong stability preserving
Runge-Kutta (SSPRK) methods
\cite{gottlieb2001strong_SSPRK}.

Although explicit ODE integrators
are simple and efficient for a wide range of
CFD problems,
the Courant-Friedrichs-Lewy (CFL) constraint
that limits physical time step in explicit
methods could make them inefficient in
notably inhomogeneous or anisotropic
transient flows,
such as wall-bounded turbulence.
Such inefficiency could be overcome by
applying implicit ODE methods with
sufficient stability.
Due to Dahlquist's second barrier
\cite{dahlquist1963special},
only second and first order linear multi-step ODE methods
could achieve $A$-stability. With the
trapezoid rule not being $L$-stable, the $L$-stable
second-order backward differentiation formula (BDF2)
is extensively adopted in transient CFD problems.
Different from multi-step methods,
the single-step implicit Runge-Kutta (IRK) methods
with multiple internal stages
can achieve higher order of accuracy while
preserving stability \cite{butcher2016ODEBook}.
Among the IRK methods, fully coupled IRK methods
could achieve optimal order given number of stages,
but they require the solution of a nonlinear
system with its dimension multiple times larger
than the ODE. The enlarged algebraic system
could be especially troublesome for its implementation in
CFD solvers.
Pazner and Persson
\cite{pazner2017stage}
made effort in efficiently solving
fully IRK methods with DG, and
Jameson \cite{jameson2017evaluation}
discussed how to adopt dual time stepping
into fully IRKs.
As a result of the difficulties in the solution
of fully IRK methods,
singly diagonally implicit Runge-Kutta (SDIRK)
methods are more commonly used in CFD, for example in
\cite{wang2017compact_VR}.
SDIRK methods have lower-triangular butcher tableau,
enabling the stage values to be solved in a sequence.
As a special case of SDIRK,
ESDIRK methods are SDIRK with an explicit first stage,
which are constructed to have
stage order of 2 while being $L$-stable
\cite{kennedy2003additiveARK,kvaerno2004singly}.
High-order ESDIRK schemes and
BDF2 have been tested in
\cite{
    bijl2002implicitBDFvESDIRK,
    wang2007implicitDGTests}
to solve flow problems,
whose results illustrate
better accuracy and higher efficiency
of high-order ESDIRK methods
compared to second order BDF2.

Apart from fully IRK and SDIRK methods,
another class of implicit RK methods,
mono implicit RK methods (MIRK)
\cite{cash1975classMIRKOrig}
have explicit internal stages and
puts all the implicitness into the final
stage, allowing the implicit system to
remain have the same number of dimensions
as the original ODE.
Cash and Signhal derived examples of
$A$-stable and $L$-stable high-order
MIRK methods in
\cite{cash1977clasMIRK1,cash1982monoMIRK2}.
Kulikov and Shindin presented a similar type
of method called nested implicit RK (NIRK)
\cite{kulikov2006familyNIRKOrig}.
Discussion was made on symmetry, stiff accuracy and
other advantageous properties of a series of Gauss type
NIRK methods.
\cite{kulikov2009adaptive}.
The major drawback of NIRK or MIRK schemes is that
the Jacobian matrices of the nonlinear algebraic problems
for each NIRK or MIRK step are
polynomials of the Jacobian of ODE's right hand side.
Cash and Singhal proposed to find MIRK methods whose
Jacobian could be factorized or approximate the Jacobian
with a factorization,
so that the Newton iteration
step could be solved by solving a series of successive linear
problems \cite{cash1982monoMIRK2}.
Kulikov and Shindin found certain factorization
approximation could sabotage the stability of
the method in
\cite{kulikov2009adaptive},
and analyzed how to choose the approximation of
Jacobian in
\cite{kulikov2007asymptotic}.
MIRK and NIRK methods are more attractive than SDIRK methods
for they require fewer inner stage solving.
Typical forth order
stiffly accurate ESDIRK requires 5 implicit internal stages to be
solved,
while MIRK and NIRK could
solve only one implicit stage.


MIRK and NIRK methods have less
implicit stages than SDIRK methods,
which gives them potential to gain better efficiency in
large scale problems.
However, current literature has seldom explored
the application of MIRK or NIRK methods in
PDE solving.
The fully IRK methods, on the other hand,
have been practiced in
finite volume \cite{jameson2017evaluation} and
DG \cite{pazner2017stage}
as mentioned before,
but their implementations
in CFD methods are significantly more
complex compared to DIRK methods.

Different from previous approaches,
the current paper aims to construct
time marching schemes based on temporal
reconstruction.
By combining polynomial approximation and
numerical quadrature rules in the direction of time,
stable, high-order accurate and efficient implicit ODE methods could be
obtained straightforwardly.
It will be shown that the new methods can be implemented
in a matrix-free style, guaranteeing their portability.
The new implicit methods are further applied to
high-order compact finite volume method to
test their capabilities in flow problems. Numerical results
illustrate that the new methods are high-order accurate
and competitive with the 4th order ESDIRK method in efficiency.




\#\#\#\#

Paper's structure:...%TODO

\section{High-order Compact Finite Volume Method}
\label{sec:CFV}

\newcommand{\U}{\mathbf{U}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\x}{\mathbf{x}}

The new time-marching method developed in
the current paper, is aimed to support
high-order finite volume method.
Therefore,
this section will provide a description of
finite volume spacial discretization,
and specify details
of the high-order finite volume scheme
used in numerical tests.

\subsection{Governing Equations}
\label{ssec:GovEq}

The compressible Navier-Stokes equations has the conservative form:
\begin{equation}
    \label{eq:NS}
    \frac{\partial \U}{\partial t} +
    (\F - \F_v) \cdot \mathbf\nabla = 0
\end{equation}
where $\U$ is vector of conservative quantities and
$\F=[\F_1,\F_2,\F_3]$,
$\F_v=[\F_{v,1},\F_{v,2},\F_{v,3}]$
are
inviscid and viscous tensors of
their flux.
In cartesian coordinates $x_k, k=1,2,3$, the components are
\begin{equation}
    \U = \begin{bmatrix}
        \rho \\ \rho u_1 \\ \rho u_2 \\ \rho u_3 \\ E
    \end{bmatrix},\ \
    \F_j = \begin{bmatrix}
        \rho u_j                   \\
        \rho u_1u_j + p\delta_{1j} \\
        \rho u_2u_j + p\delta_{2j} \\
        \rho u_3u_j + p\delta_{3j} \\
        (E+p)u_j                   \\
    \end{bmatrix},\ \
    \F_{v,j} = \begin{bmatrix}
        0                                \\
        \tau_{1j}                        \\
        \tau_{2j}                        \\
        \tau_{3j}                        \\
        \sum_{k=1}^3{u_k\tau_{kj}} - K_j \\
    \end{bmatrix}
\end{equation}
where $\rho$ is density,
$u_i, i=1,2,3$ are velocity components,
$p$ is pressure,
$E$ is total energy per unit volume,
$\tau_{ij}, i,j=1,2,3$ are viscous stress tensor components
and
$K_i, i=1,2,3$ are heat flux components.
$\delta_{ij}$ is
Kronecker delta.
With ideal gas equation of state,
Newtonian viscosity and Fourier
heat conduction, additional relations
\begin{equation}
    \begin{aligned}
        E         & = \frac{p}{\gamma -1 } + \frac{1}{2}\rho\sum_{k=1}^{3}(u_ku_k)  \\
        p         & =\rho R_g T                                                     \\
        \tau_{ij} & =
        \mu\left(\frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i}\right)
        -
        \frac{2}{3}\mu \delta_{ij}\sum_{k=1}^{3}{\frac{\partial u_k}{\partial x_k}} \\
        K_i       & = - \kappa \frac{\partial T}{\partial x_i}
    \end{aligned}
\end{equation}
are used to close the equations
with $T$ being temperature, $\gamma$ being specific heat ratio,
$R_g$ being  gas constant, $\mu$ being dynamic viscosity,
$\kappa$ being thermal conductivity.
Specific heat ratio $\gamma$ is fixed to $1.4$ in this paper.
The current paper only
considers using simple gas property
with $\kappa = \mu c_p / Pr$,
and $\mu=\mu_{\infty}$ being a constant,
while $c_p$ is special heat capacity
at constant pressure and
Prandtl number $Pr$ is fixed to 0.71 in this paper.
When $\mu=0$,
$\F_v=0$ and
equation \eqref{eq:NS} becomes Euler equation.
The equations discussed above are in 3D form, and
assuming constant distribution of values over $x_3$
yields the 2D version of NS and Euler equations.

\subsection{High-order Finite Volume Spacial Discretization}
\label{ssec:FV}

\newcommand{\OO}{\mathbf{\Omega}}
\newcommand{\UM}{\overline{\U}}
\newcommand{\Fn}{\tilde{\F}}
\newcommand{\n}{\mathbf{n}}
\newcommand{\uu}{\overline{\mathbf{U}}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\inc}{\mathrm\Delta}
\newcommand{\Tau}{\mathrm{T}}
\renewcommand{\real}{\mathrm{Re}}
\newcommand{\imag}{\mathrm{Im}}

\newcommand{\CFLt}{\text{CFL}_t}
\newcommand{\CFLtau}{\text{CFL}_\tau}

This subsection provides a general framework of
high-order
finite volume discretization.
The computational domain $\OO$ is divided
into $N_{cell}$ cells $\OO_i, i=1,2,...N_{cell}$ which
are non-overlapping, forming a mesh.
An averaging of conservative quantities
over each cell is
\begin{equation}
    \label{eq:FVMean}
    \UM_j = \frac{1}{\overline{\OO}_j}\int_{\OO_j}\U(\x)dV
\end{equation}
where $\overline{\OO}_j$ is the volume of $\OO_j$.

Next, a degree $p$ piecewise polynomial reconstruction is
conducted to approximate the distribution of
quantities
\begin{equation}
    \label{eq:FVRec}
    \U_j(\x) = \UM_j + \sum_{i=1}^{\mathrm{NDOF}(p)}{\U_j^i\varphi_{j,i}(\x) }
\end{equation}
in which $\U_j(\x)$ is the local polynomial distribution on cell $j$,
and
$\varphi_{j,i}(\x)$ are
polynomial basis functions.
The current paper uses zero-mean Taylor basis
for reconstruction \cite{wang2017compact_VR}.
Given a specific reconstruction method,
the reconstruction coefficients $\U_j^i$ can be determined using
the distribution of mean value $\UM_k, k=1,2...N_{cell}$
and boundary conditions.

With the piecewise polynomial approximation,
the PDEs \eqref{eq:NS} can therefore become ODEs
with cell averaging applied
\begin{equation}
    \label{eq:FVInt}
    \frac{\dd\UM_i}{\dd t}
    +\sum_{j\in S_i, j\neq i}{\left(
        \int_{f_{i,j}}{
            [\Fn(\U_i,\U_j) - \Fn_v(\U_i,\U_j, \nabla \U_i, \nabla \U_j)] \cdot \n  \dd A
        }\right)
    }
    = 0
\end{equation}
where $\Fn(\U_i,\U_j)$ and $\Fn_v(\U_i,\U_j, \nabla \U_i, \nabla \U_j)$ are approximations
of exact fluxes on the cell interfaces,
and $f_{i,j}=\OO_i \cap \OO_j$ is the interface between $i,j$ cells.
Set $S_i$ denotes the compact stencil of cell $i$.
The piecewise polynomial approximation \eqref{eq:FVRec} does not
guarantee a continuous distribution on interfaces,
thus the numerical fluxes
$\Fn(\U_i,\U_j)$, $\Fn_v(\U_i,\U_j, \nabla \U_i, \nabla \U_j)$ are
functions of the approximate field on both sides. Inviscid numerical
flux $\Fn(\U_i,\U_j)$ is typically an approximate Riemann solver which
will be specified for each numerical test in the paper.
Numerical viscous flux $\Fn_v(\U_i,\U_j, \nabla \U_i, \nabla \U_j)$
in this paper follows the practice of Wang \cite{wang2017compact_VR}.

As the spacial derivatives are approximated in \eqref{eq:FVInt},
it is referred to as the semi-discretized form of finite volume method.
The inviscid term has a truncation error of $O(h^{p+1})$ for smooth problems,
with $h$ being the size of mesh. As the approximate fields $\U_i$ are functions
of average values $\UM_i$, \eqref{eq:FVInt} can be rearranged into the
assembled form
\begin{equation}
    \label{eq:FVODE}
    \frac{d\uu}{dt} = \R(t, \uu)
\end{equation}
with
\begin{equation}
    \uu = \begin{bmatrix}
        \UM_1 \\
        \dots \\
        \UM_{N_{cell}}
    \end{bmatrix},\ \
    \R = \begin{bmatrix}
        -\sum_{j\in S_1, j\neq 1}{\left(
            \int_{f_{1,j}}{
                [\Fn - \Fn_v] \cdot \n  dA
            }\right)
        }     \\
        \dots \\
        -\sum_{j\in S_{N_{cell}}, j\neq N_{cell}}{\left(
            \int_{f_{N_{cell},j}}{
                [\Fn - \Fn_v] \cdot \n  dA
            }\right)
        }
    \end{bmatrix}
\end{equation}
The arguments in the numerical fluxes are omitted.
The right-hand-side vector $\R$ is also a function of $t$
as boundary conditions or additional source terms
could depend on $t$ generally speaking.




\subsection{Variational Reconstruction}
\label{ssec:VR}

% TODO
\#\#\#


\section{Direct Integration with Temporal Reconstruction (DITR) Methods}
\label{sec:HIRK}

\subsection{General Construction of DITR Methods}

Considering the first order ODE equation \eqref{eq:FVODE} arising
from high-order finite volume method
with
$t\in[0,\infty)$ and $\uu\in \mathbb{R}^N$:
\begin{equation*}
    \frac{\dd \uu}{\dd t} = \R(t, \uu)
\end{equation*}
which can be
considered a more general first order ODE with $\uu$ being
a general vector. For example, in finite difference,
$\uu$ represents the vector of grid point values.
A direct integration of equation \eqref{eq:FVODE}
leads to a time-marching relation
\begin{equation}
    \uu^{n+1} = \uu^{n} + \int_{t^n}^{t^{n+1}}{
    \R(t, \uu) \dd t
    }
    \label{eq:DI}
\end{equation}

In order to acquire the integration result in equation
\eqref{eq:DI}, a numeric quadrature rule in the interval
$[t^n, t^{n+1}]$ is used. For example, the current paper
uses three point polynomial quadrature rule
\begin{equation}
    \label{eq:Quad3}
    \begin{aligned}
        \frac{\uu^{n+1} - \uu^{n}}{\inc t^n} = & \frac{1}{\inc t^n}
        \int_{t^n}^{t^{n+1}}{
        \R(t, \uu) \dd t
        }                                                           \\ \approx &
        b_1\R(t^n, \uu(t^n))
        +
        b_2\R(t^{n+c_2}, \uu(t^{n+c_2}))
        +
        b_3\R(t^{n+1}, \uu(t^{n+1}))
    \end{aligned}
\end{equation}
where $t^{n+c_2} = t^{n} + c_2 (t^{n+1} - t^n)$ and $c_2\in(0,1)$.
$\inc t^n=t^{n+1} - t^n$ is
the time step size.
The parameter $c_2$ represents the relative place of the second abscissa
in the quadrature rule, where the first and third fixed at $t^{n}$ and $t^{n + 1}$.
Using quadratic polynomial interpolation,
the weights of the quadrature rule will be:
\begin{equation}
    \begin{aligned}
        b_1 & = \frac{1}{2} - \frac{1}{6{c_2}}     \\
        b_2 & = \frac{1}{6{c_2}(1-{c_2})}          \\
        b_3 & = \frac{1}{2} - \frac{1}{6(1-{c_2})} \\
    \end{aligned}
    \label{eq:integ0}
\end{equation}
The quadrature rule defined with
equations \eqref{eq:Quad3}, \eqref{eq:integ0} has algebraic precision
of degree 2. When $c_2=1/2$, the quadrature rule has algebraic precision of
degree 3 and becomes the three point Gauss-Lobatto rule.
The numeric integration process used in equation \eqref{eq:Quad3}
is referred to as a direct integration process because it is
directly derived from the ODE.

The precise value of $\uu(t)$ at each $t\in[t^n, t^{n+1}]$
is unknown, thus an approximation is required for numeric calculation.
The current paper proposes to use conditions at
time steps only, namely $t \in \{t^{n+1}, t^n, t^{n-1}\dots\}$,
in acquiring the polynomial reconstruction.
Since the step values $\uu^n, \uu^{n-1}\dots$ and
their temporal derivatives $\R^n, \R^{n-1}\dots$ are known,
the addition of the unknown latest step $\uu^{n+1}, \R^{n+1}$
makes the method implicit.
The current paper has only considered using a subset of
$\uu(t), \R(t, \uu(t))$ with $t \in \{t^{n+1}, t^n, t^{n-1}\}$
as polynomial conditions.
Generally, the polynomial interpolation
of $\uu(t)$ could be expressed as
\begin{equation}
    \begin{aligned}
        \label{eq:TR}
        \uu(t) & \approx
        A^n_0(t)\uu^{n - 1} +
        A^n_1(t)\uu^{n} +
        A^n_2(t)\uu^{n + 1}
        \\ & +
        \inc t^n D^n_0(t)\R^{n - 1} +
        \inc t^n D^n_1(t)\R^{n} +
        \inc t^n D^n_2(t)\R^{n + 1}
    \end{aligned}
\end{equation}
where $A^n_i(t), D^n_i(t), i=0,1,2$
are polynomial base functions. Due to the precision of quadrature rule,
the polynomial reconstruction is only expected to reach 3rd order
constructing a 4th order accurate scheme. Consequently, at most 4 of the
6 conditions would be used at the same time.
The polynomial interpolation in the direction of time in equation \eqref{eq:TR}
is referred to as temporal reconstruction, for
a continuous distribution of $\uu$ is reconstructed with point values,
similar to the finite volume reconstruction.

Combining the direct integration in equation \eqref{eq:Quad3}
and a  temporal reconstruction in \eqref{eq:TR},
a Direct Integration with Temporal Reconstruction (DITR) method is
determined.
The current paper sticks to the same formula of direct integration
in equation \eqref{eq:Quad3}, while experimenting on different
forms of temporal reconstruction in equation \eqref{eq:TR}.

If the quadrature rule in direct integration is replaced with
midpoint rule, while the temporal reconstruction uses linear
reconstruction, the implicit midpoint method for ODE can be derived,
which is 2nd order accurate. A high order accurate DITR method could
only be obtained with a high order accurate quadrature rule.
For simplicity, equation \eqref{eq:Quad3} only takes one interpolated
node besides $t^n, t^{n+1}$, and has an accuracy of 3rd or 4th order
depending on $c_2$.

Assuming the quadrature rule has algebraic precision is
of degree $m$, and the polynomial interpolation is of degree $n$,
a straightforward analysis on local truncation error could be conducted.
Approximation \eqref{eq:Quad3} yields a
truncation error of $O((\inc t)^{m+2} )$ expressed in equation \eqref{eq:Quad3Err}
\begin{equation}
    \label{eq:Quad3Err}
    \begin{aligned}
        \uu^{n+1} - \uu^{n} = & \int_{t^n}^{t^{n+1}}{
        \R(t, \uu)\dd t}                              \\  = &
        {\inc t}^{n}
        \left[
            b_1\R(t^n, \uu(t^n))
            +
            b_2\R(t^{n+c_2}, \uu(t^{n+c_2}))
            +
            b_3\R(t^{n+1}, \uu(t^{n+1}))
            \right]
        \\ + &
        O((\inc t^{n})^{m+2} )
    \end{aligned}
\end{equation}
due to the precision degree of quadrature rule.
Approximation
\eqref{eq:TR} has a truncation error of $O((\inc t)^{n+1})$
expressed in \eqref{eq:TRErr}
\begin{equation}
    \begin{aligned}
        \label{eq:TRErr}
        \uu(t^{n+c_2}) & =
        A^n_0(t^{n+c_2})\uu^{n - 1} +
        A^n_1(t^{n+c_2})\uu^{n} +
        A^n_2(t^{n+c_2})\uu^{n + 1}
        \\ & +
        \inc t^n D^n_0(t^{n+c_2})\R^{n - 1} +
        \inc t^n D^n_1(t^{n+c_2})\R^{n} +
        \inc t^n D^n_2(t^{n+c_2})\R^{n + 1}
        \\ & +
        O((\inc t^{n})^{n+1} )
    \end{aligned}
\end{equation}
as a result of polynomial degree.
Substituting \eqref{eq:TRErr} into the $t^{n+c_2}$ stage
of \eqref{eq:Quad3Err}, the truncation error
of the entire scheme becomes $O((\inc t)^{n+2}) + O((\inc t)^{m+2})$:
\begin{equation}
    \begin{aligned}
        \uu^{n+1} = & \uu^{n} + {\inc t}^{n}
        \left[
            b_1\R(t^n, \uu^n)
            +
            b_2\R(t^{n+c_2}, \uu^{n+c_2} + O((\inc t^{n})^{n+1} ))
            +
            b_3\R(t^{n+1}, \uu^{n+1})
            \right]
        \\ + &
        O((\inc t^{n})^{m+2} )               \\
        =           &
        \uu^{n} + {\inc t}^{n}
        \left[
            b_1\R(t^n, \uu^n)
            +
            b_2\R(t^{n+c_2}, \uu^{n+c_2})
            +
            b_3\R(t^{n+1}, \uu^{n+1})
            \right]
        \\ + &
        O((\inc t^{n})^{m+2}  + O((\inc t^{n})^{n+2} ))
    \end{aligned}
    \label{eq:fullLTE}
\end{equation}
where $\uu^{n+1}, \uu^{n}$ are step values assumed to be accurate here, and
$\uu^{n+c_2}$ is the approximate
$c_2$ stage value determined with temporal reconstruction.
Equation \eqref{eq:fullLTE} assumes
$\R(t, \uu)$ to be sufficiently regular and therefore does not
change the order of error.
Therefore, for smooth problems the
order of accuracy of a DITR
method is theoretically $\min(m,n) + 1$.

The following sections will illustrate some
practical DITR methods based on equations \eqref{eq:Quad3} and \eqref{eq:TR}.


\subsection{Variants of DITR Methods}



\subsubsection{The DITR U2R2 Method}

To make the method single-step and 4th order accurate,
we choose $\uu^{n},\uu^{n+1}$, $\R^{n},\R^{n+1}$ as
the interpolation conditions, making the interpolation
basically cubic Hermite interpolation.
Therefore, we have the interpolation of $\uu(t^{n+c_2})$
being:
\begin{equation}
    \begin{aligned}
        \label{eq:TRU2R2}
        \uu^{n+c_2} & =
        a_{1,U2R2}\uu^{n} +
        a_{2,U2R2}\uu^{n + 1}
        \\ & +
        \inc t^n d_{1,U2R2}\R^{n} +
        \inc t^n d_{2,U2R2}\R^{n + 1}
    \end{aligned}
\end{equation}
with $\uu^{n+c_2}$ being
the numerical approximation
of $\uu(t^{n+c_2})$
and the interpolation bases at $c_2$ node being:
\begin{equation}
    \begin{aligned}
        a_{1,U2R2} & = 1 - (3{c_2}^2 - 2 {c_2}^3)  \\
        a_{2,U2R2} & = 3{c_2}^2 - 2 {c_2}^3        \\
        d_{1,U2R2} & = {c_2} - 2 {c_2}^2 + {c_2}^3 \\
        d_{2,U2R2} & = - {c_2}^2 + {c_2}^3         \\
    \end{aligned}
    \label{eq:interpU2R2}
\end{equation}

The temporal reconstruction equation \eqref{eq:TRU2R2}
combined with direct integration equation \eqref{eq:Quad3}
forms the DITR U2R2 method.

When $c_2=1/2$, quadrature rule in equation \eqref{eq:Quad3} has precision
of degree 3, making the DITR U2R2 method 4th order accurate,
and the stage value $\uu^{n+c_2}$ has a precision of degree 3.
When $c_2\neq1/2$, DITR U2R2 becomes 3rd order accurate.

In order to further examine the accuracy order,
equations \eqref{eq:interpU2R2}, \eqref{eq:Quad3}
can be reformulated into a standard IRK method,
yielding a Butcher tableau shown in Table \ref{tab:U2R2Butcher}.
\begin{table}[htbp]
    \centering
    \begin{tabular}{c|ccc}
        0     & 0              & 0        & 0              \\
        $c_2$ & $d_1 + a_2b_1$ & $a_2b_2$ & $d_2 + a_2b_3$ \\
        1     & $b_1$          & $b_2$    & $b_3$          \\ \hline
              & $b_1$          & $b_2$    & $b_3$
    \end{tabular}
    \caption{Butcher tableau of DITR U2R2}
    \label{tab:U2R2Butcher}
\end{table}

From table \ref{tab:U2R2Butcher} with the coefficients
decided with \eqref{eq:interpU2R2} and \eqref{eq:integ0},
one can find that the 4th order accurate
DITR U2R2 method with $c_2=1/2$
is actually the Lobatto IIIA method
of order 4 \cite{wanner1996solving}.
The classic order and stage order of DITR U2R2 could
also be verified using Table \ref{tab:U2R2Butcher} via
the simplifying assumptions, which is a trivial procedure
given the formulae provided in \cite{wanner1996solving}.

Following standard analysis based on Dahlquist's equation
$\frac{dy}{dt} = \lambda y$ \cite{wanner1996solving},
the stability function giving by $y^{1}=R(h\lambda)y^0$
applied to DITR U2R2 is in the form:
\begin{equation}
    \label{eq:stabilityFuncU2R2}
    R_{U2R2}(z) = -\frac{4\,z-2\,c_{2}\,z-c_{2}\,z^2+z^2+6}{2\,z+2\,c_{2}\,z-c_{2}\,z^2-6}
\end{equation}
which becomes the (2,2)-Pad{\'e} approximation when $c_2=1/2$ and
DITR U2R2 becomes Lobatto IIIA.
Analysis on equation \eqref{eq:stabilityFuncU2R2}
would show that $c_2\in[1/2,1)$ is a sufficient and necessary
condition of DITR U2R2 being $A$-stable given $c_2\in(0,1)$.
The limit at infinity
\begin{equation}
    \lim_{z\rightarrow\infty}R_{U2R2}(z) = \frac{1-c_2}{c_2}
\end{equation}
confirm that DITR U2R2 is unable to achieve $L$-stability
by adjusting $c_2$.


DITR U2R2 ($c_2=1/2$) or Lobatto IIIA method is symmetric,
which is a preferable property when integrating
reversible systems including orbital motion and particle
systems.
However,
the symmetry in this ODE method could be considered harmful in CFD application.
Most CFD systems of interest are physically dissipative,
while for a symmetric method
$R(z) \rightarrow 1$ when $z \rightarrow \infty$,
which means the method is more likely to preserve
spurious modes arising from spacial discretization.
Although DITR U2R2 cannot achieve $L$-stability,
using a value of $c_2 > 1/2$ would still
produce $\lim_{z\rightarrow\infty}R(z)\in(0,1)$, which
would be a useful property in simulation of dissipative systems.
With  $c_2 > 1/2$, stiff modes could vanish faster over the time
steps, while $c_2 = 1/2$ tends to preserve them.

\subsubsection{The DITR U2R1 Method}

Giving up one interpolation condition in DITR U2R2
forces the scheme to have 3rd order accuracy.
The current paper removes $\R^{n}$ from U2R2, namely
using $\uu^{n},\uu^{n+1}$, $\R^{n+1}$ for interpolation,
which is able to produce an $L$-stable DITR scheme.

Similar with U2R2, DITR U2R1 has the interpolation
written as:
\begin{equation}
    \begin{aligned}
        \label{eq:TRU2R1}
        \uu^{n+c_2} & =
        a_{1,U2R1}\uu^{n} +
        a_{2,U2R1}\uu^{n + 1}
        \\ & +
        \inc t^n d_{2,U2R1}\R^{n + 1}
    \end{aligned}
\end{equation}
with $\uu^{n+c_2}$ being
the numerical approximation
of $\uu(t^{n+c_2})$
and the interpolation bases at $c_2$ node being:
\begin{equation}
    \begin{aligned}
        a_{1,U2R1} & = 1 - (2c_2 - {c_2}^2) \\
        a_{2,U2R1} & = 2c_2 - {c_2}^2       \\
        d_{2,U2R1} & = {c_2}^2 - {c_2}      \\
    \end{aligned}
    \label{eq:interpU2R1}
\end{equation}

The temporal reconstruction equation \eqref{eq:TRU2R1}
combined with direct integration equation \eqref{eq:Quad3}
forms the DITR U2R1 method.

The interpolation bases shown in \eqref{eq:interpU2R1}
are quadratic.
Therefore, the scheme yields 3rd order accuracy
and the choice of $c_2$ does not affect the order of accuracy.
Similar to U2R2, DITR U2R1's order of accuracy can
be examined using
standard procedures for
Runge-Kutta methods\cite{wanner1996solving}.

The linear stability function for DITR U2R1 is
\begin{equation}
    \label{eq:stabilityFuncU2R1}
    R_{U2R1}(z) = \frac{2\,z+6}{z^2-4\,z+6}
\end{equation}
which is (1,2)-Pad{\'e} approximation and not affected by $c_2$.
It can be found $|R_{U2R1}(z)| < 1, \forall \real(z) < 0$,
and obviously $\lim_{z\rightarrow\infty}R_{U2R1}(z) = 0$.
Therefore, DITR U2R1 method is $L$-stable.

The stability function also
shows that for a linear $\R$, the solution is not
affected by $c_2$, but U2R1 treats nonlinear problems
different when $c_2$ changes.

\subsubsection{The DITR U3R1 Method}

To exploit the information when multiple previous
steps are available,
using conditions from $t^{n-1}$ would be
preferable.
The current paper chooses $\uu^{n-1},\uu^{n}$, $\uu^{n+1}$
and $\R^{n+1}$ as U3R1's interpolation conditions,
as other choices do not produce sufficient linear stability.
The interpolation at $c_2$ node becomes:
\begin{equation}
    \begin{aligned}
        \label{eq:TRU3R1}
        \uu^{n+c_2} & =
        a_{0,U3R1}\uu^{n} +
        a_{1,U3R1}\uu^{n} +
        a_{2,U3R1}\uu^{n + 1}
        \\ & +
        \inc t^n d_{2,U3R1}\R^{n + 1}
    \end{aligned}
\end{equation}
with $\uu^{n+c_2}$ being
the numerical approximation
of $\uu(t^{n+c_2})$
and the interpolation bases at $c_2$ node being:
\begin{equation}
    \begin{aligned}
        a_{0,U3R1} & = -\frac{c_{2}\,{\left(c_{2}-1\right)}^2}{\Theta\,{\left(\Theta+1\right)}^2}                                                            \\
        a_{1,U3R1} & = \frac{\left(\Theta+c_{2}\right)\,{\left(c_{2}-1\right)}^2}{\Theta}                                                                    \\
        a_{2,U3R1} & =  \frac{c_{2}\,\left(-\Theta^2\,c_{2}+2\,\Theta^2-\Theta\,{c_{2}}^2+3\,\Theta-2\,{c_{2}}^2+3\,c_{2}\right)}{{\left(\Theta+1\right)}^2} \\
        d_{2,U3R1} & =   \frac{c_{2}\,\left(\Theta+c_{2}\right)\,\left(c_{2}-1\right)}{\Theta+1}
    \end{aligned}
    \label{eq:interpU3R1}
\end{equation}
where $\Theta = \inc t^{n-1}/\inc t^{n}$.

The temporal reconstruction equation \eqref{eq:TRU3R1}
combined with direct integration equation \eqref{eq:Quad3}
forms the DITR U2R1 method.

The method is 4th order when $c_2=1/2$, when
both the interpolation and integration has precision
of degree 3.
For the special case of $c_2=1/2$, simplified
coefficients are given:
\begin{equation}
    \begin{aligned}
        a_{0,U3R1} & = -\frac{1}{8\,\Theta\,{\left(\Theta+1\right)}^2}                \\
        a_{1,U3R1} & = \frac{\Theta+\frac{1}{2}}{4\,\Theta}                           \\
        a_{2,U3R1} & =  \frac{6\,\Theta^2+11\,\Theta+4}{8\,{\left(\Theta+1\right)}^2} \\
        d_{2,U3R1} & = -\frac{\Theta+\frac{1}{2}}{4\,\left(\Theta+1\right)}
    \end{aligned}
    \label{eq:interpU3R1-S}
\end{equation}

Linear stability is analyzed when $\Theta =1$ and $c_2 = 1/2$.
The solution to the test problem
produces two solutions:
\begin{equation}
    \left(
    \begin{matrix}
        R_{U3R1}^{(1)}(z) \\
        R_{U3R1}^{(2)}(z)
    \end{matrix}
    \right)=\left(\begin{array}{c} \frac{10\,z-\sqrt{-6\,z^3+129\,z^2+432\,z+576}+24}{6\,z^2-29\,z+48}\\ \frac{10\,z+\sqrt{-6\,z^3+129\,z^2+432\,z+576}+24}{6\,z^2-29\,z+48} \end{array}\right)
\end{equation}
It is obvious $R_{U3R1}^{(1)}(z)\rightarrow 0, R_{U3R1}^{(2)}(z)\rightarrow0$ when
$z\rightarrow\infty$.
When observed numerically, it is found both $|R_{U3R1}^{(1)}(z)|$
and $|R_{U3R1}^{(2)}(z)|$ are less than 1 in the left
half plane of $z$.
The current paper therefore believes %! not a linear multi-step method
the DITR U3R1 method is indeed $L$-stable.
Note that DITR U3R1 is not a linear multistep method,
and the root locus curve analysis used in those methods
can not be directly applied here.

With 4th order accuracy and observed $L$-stability, the
DITR U3R1 method is potentially more favorable than
U2R2 and U2R1.

\subsubsection{Summary of DITR Methods}

The U2R2, U2R1 and U3R1 variants of the
DITR method can be written is a unified form.
The first equation is the direct integration:
\begin{equation}
    \uu^{n+1} = \uu^{n} + \inc t^n\left(
    b_1 \R^n +
    b_2 \R^{n+c_2} +
    b_3 \R^{n+1}
    \right)
    \label{eq:DISum}
\end{equation}
with weights decided by \eqref{eq:integ0},
and also listed Table \ref{tab:integ0Tab}.
For simplicity, in numerical expressions,
notations such as $\R^{n+c_2}=\R(t^{n+c_2}, \uu^{n+c_2})$
are used from now on.
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        $b_1$                            & $b_2$ & $b_3$ \\
        \hline
        $\frac{1}{2} - \frac{1}{6{c_2}}$ &
        $\frac{1}{6{c_2}(1-{c_2})}$      &
        $\frac{1}{2} - \frac{1}{6(1-{c_2})} $            \\
        \hline
    \end{tabular}
    \caption{Butcher tableau of DITR U2R2}
    \label{tab:integ0Tab}
\end{table}

The second equation is the temporal reconstruction:
\begin{equation}
    \label{eq:TRSum}
    \uu^{n+c_2}  =
    a^n_0\uu^{n - 1} +
    a^n_1\uu^{n} +
    a^n_2\uu^{n + 1}
    +
    \inc t^n
    \left(
    d^n_1\R^{n} +
    d^n_2\R^{n + 1}
    \right)
\end{equation}
where the coefficients vary in different schemes
following Table \ref{tab:inter0Tab} where
$\Theta = \inc t^{n-1} / \inc t^{n}$.
% \begin{table}[htbp]
%     \centering
%     \footnotesize
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         DITR Method                                                                                                                            & $a^n_0$ & $a^n_1$ & $a^n_2$ & $d^n_1$ & $d^n_2$ \\
%         \hline
%         U2R2                                                                                                                                   &
%         0                                                                                                                                      &
%         $1-(3{c_2}^2 - 2 {c_2}^3)$                                                                                                             &
%         $3{c_2}^2 - 2 {c_2}^3$                                                                                                                 &
%         ${c_2} - 2 {c_2}^2 + {c_2}^3$                                                                                                          &
%         $- {c_2}^2 + {c_2}^3$                                                                                                                                                                    \\
%         \hline
%         U2R1                                                                                                                                   &
%         0                                                                                                                                      &
%         $1-(2c_2 - {c_2}^2)$                                                                                                                   &
%         $2c_2 - {c_2}^2$                                                                                                                       &
%         0                                                                                                                                      &
%         ${c_2}^2 - {c_2} $                                                                                                                                                                       \\
%         \hline
%         U3R1                                                                                                                                   &
%         $-\frac{c_{2}\,{\left(c_{2}-1\right)}^2}{\Theta\,{\left(\Theta+1\right)}^2}$                                                           &
%         $\frac{\left(\Theta+c_{2}\right)\,{\left(c_{2}-1\right)}^2}{\Theta}$                                                                   &
%         $\frac{c_{2}\,\left(-\Theta^2\,c_{2}+2\,\Theta^2-\Theta\,{c_{2}}^2+3\,\Theta-2\,{c_{2}}^2+3\,c_{2}\right)}{{\left(\Theta+1\right)}^2}$ &
%         0                                                                                                                                      &
%         $\frac{c_{2}\,\left(\Theta+c_{2}\right)\,\left(c_{2}-1\right)}{\Theta+1}$                                                                                                                \\
%         \hline
%     \end{tabular}
%     \caption{Butcher tableau of DITR U2R2}
%     \label{tab:inter0Tab}
% \end{table}

\begin{table}[htbp]
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|}
        \hline
        DITR Method & U2R2                          & U2R1                 & U3R1                                                                                                                                   \\
        \hline
        $a^n_0$     & 0                             & 0                    & $-\frac{c_{2}\,{\left(c_{2}-1\right)}^2}{\Theta\,{\left(\Theta+1\right)}^2}$                                                           \\
        \hline
        $a^n_1$     & $1-(3{c_2}^2 - 2 {c_2}^3)$    & $1-(2c_2 - {c_2}^2)$ & $\frac{\left(\Theta+c_{2}\right)\,{\left(c_{2}-1\right)}^2}{\Theta}$                                                                   \\
        \hline
        $a^n_2$     & $3{c_2}^2 - 2 {c_2}^3$        & $2c_2 - {c_2}^2$     & $\frac{c_{2}\,\left(-\Theta^2\,c_{2}+2\,\Theta^2-\Theta\,{c_{2}}^2+3\,\Theta-2\,{c_{2}}^2+3\,c_{2}\right)}{{\left(\Theta+1\right)}^2}$ \\
        \hline
        $d^n_1$     & ${c_2} - 2 {c_2}^2 + {c_2}^3$ & 0                    & 0                                                                                                                                      \\
        \hline
        $d^n_2$     & $- {c_2}^2 + {c_2}^3$         & ${c_2}^2 - {c_2}$    & $\frac{c_{2}\,\left(\Theta+c_{2}\right)\,\left(c_{2}-1\right)}{\Theta+1}$                                                              \\
        \hline
    \end{tabular}
    \caption{Interpolation coefficients for different DITR methods}
    \label{tab:inter0Tab}
\end{table}



% \subsection{Time Marching Based on Interpolation}

% The acquiring of an NIRK method with
% certain order of accuracy is usually based on
% a certain integration method and
% corresponding taylor expansion analysis,
% as described in \cite{kulikov2006familyNIRKOrig}.
% However, the present paper proposes a single step method
% derived with Hermite interpolation
% (thus called HIRK),
% which simplifies the acquiring of high
% stage order and provides an approach to
% modify the scheme's stability.

% Considering the first order ODE arising from high-order finite volume method
% with
% $t\in[0,\infty)$ and $\uu\in \mathbb{R}^N$:
% \begin{equation*}
%     \frac{d\uu}{dt} = \R(t, \uu)
% \end{equation*}
% which is directly taken from equation \eqref{eq:FVODE}.
% A direct integration of equation \eqref{eq:FVODE}
% leads to a time-marching relation
% \begin{equation}
%     \uu^{n+1} = \uu^{n} + \int_{t^n}^{t^{n+1}}{
%     \R(t, \uu)
%     }
% \end{equation}

% With $\uu^n$ as the known numerical solution at $t=t^n$ and
% $t^{n+1} = t^{n} + \inc t$,
% we first consider the construction of implicit second order
% time marching methods as an introduction.
% First, form a linear interpolation for $t\in[t^n, t^{n+1}]$,
% which gives an approximate time-distribution of
% $\uu(t)=\frac{t^{n+1} - t}{\inc t}\uu^n + \frac{t - t^n}{\inc t}\uu^{n+1}$.
% Second, use a second order accurate quadrature rule on
% the linear time distribution. Evidently a trapezoid rule yields
% trapezoid rule method or Crank-Nicolson method, and
% midpoint rule yields the second order midpoint method.
% Inspired by this procedure, now we proceed to construct
% a high-order version.

% For a single step implicit method,
% if the implicitness is already solved,
% the endpoint values $\uu^n, \uu^{n+1}$ are
% given. Moreover,  $\R^n, \R^{n+1}$ can be
% directly evaluated, where $\R^n = \R(t^n, \uu^n)$.
% Given the endpoint values and corresponding first derivatives,
% an Hermite interpolation can be given:
% \newcommand{\ttt}{t^*}
% \begin{equation}
%     \label{eq:Hermite}
%     \begin{aligned}
%         \uu(t) & \approx (1-3{\ttt}^2 - 2 {\ttt}^3)\uu^n
%         + (3{\ttt}^2 - 2 {\ttt}^3)\uu^{n+1}                     \\
%                & + \inc t ({\ttt} - 2 {\ttt}^2 + {\ttt}^3) \R^n
%         + \inc t (- {\ttt}^2 + {\ttt}^3) \R^{n+1}
%     \end{aligned}
% \end{equation}
% where $\ttt = (t - t^n)/\inc t$ and $\ttt \in [0,1]$.
% The Hermite interpolation \eqref{eq:Hermite}
% has $O(\inc t^4)$ truncation error.
% Next, a quadrature rule is applied.
% To utilize the endpoints and reduce memory requirement,
% we only insert one middle stage at $\ttt = c_2$.
% Using polynomial interpolation based
% numerical integration, with nodes $\ttt = 0,c_2,1$,
% the numerical integration of some function $r(t)$ would be
% \begin{equation}
%     \label{eq:Quad}
%     \begin{aligned}
%         \int_{t^n}^{t^{n+1}}r(t)dt & \approx \\
%         \left(\frac{1}{2} - \frac{1}{6{c_2}}\right)r(t^n)
%                                    & +
%         \left(\frac{1}{6{c_2}(1-{c_2})}\right)r(t^n + c_2\inc t )
%         +
%         \left(\frac{1}{2} - \frac{1}{6(1-{c_2})}\right)r(t^{n+1})
%     \end{aligned}
% \end{equation}
% using the relation $\int_{t^n}^{t^{n+1}}\R dt = \uu^{n+1} -\uu^{n}$,
% there are two algebraic relations and two unknown vectors
% in total, which is able to form a solvable system.
% The quadrature rule \eqref{eq:Quad} has truncation error
% $O(\inc t^3)$ at least,
% for it is based on second degree polynomial interpolation.


% Summarizing the discussions above, the HIRK method has the form:
% \begin{subequations}
%     \label{eq:HM3}
%     \begin{align}
%         \uu^{n+1} & = \uu^{n} +
%         \inc t
%         \left[
%             b_1\R(t^{n,1}, \uu^n) +
%             b_2\R(t^{n+c_2}, \uu^*) +
%             b_3\R(t^{n,3}, \uu^{n+1})
%         \right]   \label{eq:HM3-1} \\
%         \uu^{*}   & =
%         a_1\uu^{n} +
%         a_2\uu^{n+1} +
%         \inc t
%         \left[
%             d_1\R(t^{n,1}, \uu^n) +
%             d_2\R(t^{n,3}, \uu^{n+1})
%             \right] \label{eq:HM3-2}
%     \end{align}
% \end{subequations}
% where $t^{n,i}=t^n+c_i\inc t$, and $c_1 = 0, c_3 = 1, c_2\in(0,1)$.

% The only stage value besides endpoints is $\uu^*$.
% The internal stage $\uu^*$ can be explicitly
% calculated with \eqref{eq:HM3-2}, which is derived
% directly from the cubic Hermite interpolation of $\uu$
% on interval $[t^n,t^{n+1}]$ evaluated at
% $t^{n+c_2}$, which result in the interpolation relation:
% \begin{equation}
%     \begin{aligned}
%         a_2 & = 1 - a_1 = 3{c_2}^2 - 2 {c_2}^3 \\
%         d_1 & = {c_2} - 2 {c_2}^2 + {c_2}^3    \\
%         d_2 & = - {c_2}^2 + {c_2}^3            \\
%     \end{aligned}
%     \label{eq:interp}
% \end{equation}
% The first equation \eqref{eq:HM3-1} is
% a numeric integration on interval $[t^n,t^{n+1}]$ with
% polynomial nodes
% $t^{n,1}=t^n,t^{n+c_2}=t^n+c_2\inc t,t^{n,3}=t^{n+1}$, which demands:
% \begin{equation}
%     \begin{aligned}
%         b_1 & = \frac{1}{2} - \frac{1}{6{c_2}}     \\
%         b_2 & = \frac{1}{6{c_2}(1-{c_2})}          \\
%         b_3 & = \frac{1}{2} - \frac{1}{6(1-{c_2})} \\
%     \end{aligned}
%     \label{eq:integ}
% \end{equation}
% The Hermite interpolation gives \eqref{eq:HM3-2}
% local truncation error $O(\inc t^4)$,
% and the numeric integration gives \eqref{eq:HM3-1}
% local truncation error $O(\inc t^3)$, therefore the
% classic order of accuracy of HIRK \eqref{eq:HM3} is
% 3. If $c_2=1/2$, then the numeric integration \eqref{eq:HM3-2}
% becomes a Gauss-Lobatto quadrature rule, which yields
% a local truncation error of $O(\inc t^4)$, making
% the scheme 4th order accurate.
% Also, from the interpolation, no matter
% the choice of $c_2$, HIRK has a stage order of
% 3, making it stiffly accurate.



% \subsection{Order of Accuracy}
% To discuss the order of accuracy and stage order
% formally, HIRK can be considered as a standard IRK.
% Reformulating \eqref{eq:HM3} into
% a standard Runge-Kutta form yields a Butcher
% tableau:
% \begin{table}[htbp]
%     \centering
%     \begin{tabular}{c|ccc}
%         0     & 0              & 0        & 0              \\
%         $c_2$ & $d_1 + a_2b_1$ & $a_2b_2$ & $d_2 + a_2b_3$ \\
%         1     & $b_1$          & $b_2$    & $b_3$          \\ \hline
%               & $b_1$          & $b_2$    & $b_3$
%     \end{tabular}
%     \caption{Butcher tableau of \eqref{eq:HM3}}
%     \label{tab:HM3Butcher}
% \end{table}

% From table \ref{tab:HM3Butcher} with the coefficients
% decided with \eqref{eq:interp} and \eqref{eq:integ},
% one can find that the 4th order accurate
% HIRK $c_2=1/2$ method is indeed the Lobatto IIIA method
% of order 4 \cite{wanner1996solving}.
% The classic order and stage order of HIRK could
% also be evaluated from table \ref{tab:HM3Butcher} via
% the simplifying assumptions, which is a trivial procedure
% given the formulae provided in \cite{wanner1996solving}.

% If the quadrature rule in HIRK is
% replaced with Gauss-Legendre rule,
% the method immediately becomes a special case of the
% Gauss type NIRK method of order 4
% described in \cite{kulikov2006familyNIRKOrig}.
% For large scale CFD application,
% using 2 point Gauss-Legendre rule
% would mean deriving the right hand side at
% 3 different stages iteratively.
% Thus, the current HIRK method only considers
% a 3 point Gauss-Lobatto type quadrature
% (with the middle abscissa moved and order of accuracy reduced),
% which would provide sufficient accuracy and only require 2 unknown stages.



% \begin{equation}
%     \label{eq:HM3R}
%     \begin{aligned}
%         \uu^{n+1} & = \uu^{n} +
%         \inc t
%         \left(
%         b_1\R(t^{n,1}, \uu^n) +
%         b_2\R(t^{n+c_2}, \uu^*) +
%         b_3\R(t^{n,3}, \uu^{n+1})
%         \right) \\
%         \uu^{*}   & =
%         \uu^{n}  + 
%         \inc t
%         \left(
%         (d_1 + a_2b_1)\R(t^{n,1}, \uu^n) +
%         a_2b_2\R(t^{n+c_2}, \uu^*) +
%         (d_2 + a_2b_3)\R(t^{n,3}, \uu^{n+1})
%         \right)
%     \end{aligned}
% \end{equation}

% \subsection{Linear Stability}
% \label{ssec:linStab}

% Following standard analysis based on Dahlquist's equation
% $\frac{dy}{dt} = \lambda y$ \cite{wanner1996solving},
% the stability function giving $y^{1}=R(h\lambda)y^0$
% when applying HIRK is in the form:
% \begin{equation}
%     \label{eq:stabilityFunc}
%     R(z) = -\frac{4\,z-2\,c_{2}\,z-c_{2}\,z^2+z^2+6}{2\,z+2\,c_{2}\,z-c_{2}\,z^2-6}
% \end{equation}
% which becomes the (2,2)-Pad{\'e} approximation when $c_2=1/2$ and HIRK
% becomes Lobatto IIIA. The limit
% \begin{equation}
%     \lim_{z\rightarrow\infty}R(z) = \frac{1-c_2}{c_2}
% \end{equation}
% gives that a necessary condition for $A$-stability of
% HIRK is $c_2 \in [1/2,1)$, and shows that HIRK
% is unable to achieve $L$-stability.
% Further analysis on \eqref{eq:stabilityFunc}
% would confirm $c_2 \in [1/2,1)$ is a sufficient
% condition for $A$-stability.

% HIRK($1/2$) or Lobatto IIIA method is symmetric,
% which is a preferable property when integrating
% reversible systems,
% but the symmetry could be considered harmful in CFD application.
% Most CFD systems of interest are physically dissipative,
% while for symmetric RK methods
% $R(z) \rightarrow 1$ when $z \rightarrow \infty$,
% which is more likely to preserve
% spurious modes arising from spacial discretization.
% Although HIRK cannot achieve $L$-stability
% by simply adjusting $c_2$,
% using a non-trivial value $c_2 > 1/2$ would still
% produce $\lim_{z\rightarrow\infty}R(z)\in(0,1)$, which
% is a useful property in simulation of dissipative systems.
% With  $c_2 > 1/2$, stiff modes could better vanish over the time
% steps, while $c_2 = 1/2$ tends to preserve them.


\subsection{Solving Approaches}
\renewcommand{\Res}{\mathcal{R}}
\newcommand{\Jres}{\mathcal{J}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\FF}{\mathcal{F}}

The DITR methods discussed in the current paper
consist of two implicit equations \eqref{eq:DISum} and
\eqref{eq:TRSum}, which are results of direct integration
and temporal reconstruction respectively.
There are two unknown vectors $\uu^{n+1},\uu^{n+c_2}$ in the equations.
In order to numerically conduct time marching with DITR, the equations
need to be iteratively solved.
The current section will discuss
how to solve the equations arising from DITR methods
when applied to spacial discretized flow problems, and
describe a recommended solving approach in the end.

\subsubsection{Dual Time Stepping}
\label{sssec:dualTime}

Dual time stepping \cite{jameson1991time,jameson2017evaluation}
is a standard procedure of solving nonlinear
implicit time stepping equations
in flow problems.
The nonlinear equations are written in the form of
\begin{equation}
    \FF^*(\uu^*) = 0
    \label{eq:abstractImplicit}
\end{equation}
where $\uu^*$ represent one of the unknowns
in DIRK, BDF or DITR methods.
For example, for trapezoid rule (Crank-Nicolson method),
equation \eqref{eq:abstractImplicit} reads
\begin{equation}
    \label{eq:trapF}
    \FF(\uu^{n+1}) =
    -\frac{\uu^{n+1}-\uu^{n}}{\inc t^n} + \frac{\R^{n} + \R^{n+1}}{2}
    =0
\end{equation}
Generally, $\FF^*$ should be arranged so that
$\R(\uu^*)$ in $\FF^*$ has coefficient being $O(1)$ and positive.
With the idea of dual time stepping, namely
\begin{equation}
    \frac{\dd\uu^{*}}{\dd \tau} = \FF^*(\uu^{*,m+1})
    \label{eq:exactDualTime}
\end{equation}
combined with a backward Euler method on the
pseudo time $\tau$, equation \eqref{eq:abstractImplicit}
is solved with
\begin{equation}
    \frac{\uu^{*,m+1} - \uu^{*,m}}{\inc \tau} = \FF^*(\uu^{*,m+1})
\end{equation}
where $m, m+1$ are pseudo time steps.
Clearly $\tau$ has a dimension of time.
Further linearizing would produce
\begin{equation}
    \left(\frac{\eye}{\inc\tau} -
    \partialderivative{\FF^*}{\uu^*}  \right)
    \inc\uu^{*,m} = \FF^*(\uu^{*,m})
    \label{eq:abstractNewton}
\end{equation}
where $\inc\uu^{*,m} = \uu^{*,m+1} - \uu^{*,m}$ is the increment
of the iteration.
The Jacobian of residual $\partialderivative{\FF^*}{\uu^*}$
is a non-trivial matrix, thus each update of $\uu^*$ requires
the solution of a sparse linear problem.
When $\inc\tau\rightarrow0$, Equation \eqref{eq:abstractNewton}
tends to reflect the exact evolution of dual time stepping
equation \eqref{eq:exactDualTime}.
Equation \eqref{eq:abstractNewton} becomes the
standard Newton-Raphson method when $\inc\tau\rightarrow\infty$.


For DIRK type methods and linear multistep methods, at
each step or stage, only one $\uu^{*}$ is unknown,
thus equation \eqref{eq:abstractNewton} can be applied directly.
However, in fully implicit RK methods and the current DITR methods,
more than one unknown $\uu^{*}$ are present at each stage.
For example, in DITR methods, both $\uu^{n+1},\uu^{n+c_2}$
are present in both equations \eqref{eq:DISum} and \eqref{eq:TRSum}.
Note that in equations \eqref{eq:DISum} and \eqref{eq:TRSum},
$\R^{n+1},\R^{n+c_2}$ are also dependent on $\uu^{n+1},\uu^{n+c_2}$.
Assuming $\R$ to be nonlinear, linear combinations of
equations \eqref{eq:DISum} and \eqref{eq:TRSum} are
impossible to completely eliminate
the dependence on one of $\uu^{n+1},\uu^{n+c_2}$, and
fully implicit RK methods share the same issue.

One of the most obvious solutions to the multiple unknowns
in DITR and fully implicit RK methods is to apply
equation \eqref{eq:abstractNewton} to an enlarged
system.
The enlarged system concatenates different $\uu$
stages as one vector and uses the concatenated vector as
$\uu^*$ in \eqref{eq:abstractNewton}.
The residual function $\FF^*$ is therefore a concatenated vector of
different stage equations' residual vectors.
Therefore, for methods with $k$ unknown stages, the number of
unknowns solved concurrently is $k$ times of that in BDF or ESDIRK methods.
For solvers that explicitly implement sparse matrix storage
and solving methods, such enlargement is trivial at the cost of $k^2$ times
of matrix storage. But for inherently matrix-free flow solvers,
such enlargement could require drastic modification and addition to
the original codebase.
The direct enlargement of the problem is referred to as
the fully coupled approach in the current paper.
Due to its complexity of implementation,
the current research does not consider using fully coupled approach
to solve DITR time stepping.




\subsubsection{DITR's Nested Solving Approach}

A characteristic of equation \eqref{eq:TRSum} is that
$\uu^{n+c_2}$ only exists on the left side, namely the
right side of equation \eqref{eq:TRSum} has only $\uu^{n+1}$ as
unknown. Consequently, by substitution, equivalent
with the previous discussions causing equation \eqref{eq:fullLTE},
$\uu^{n+c_2}$ can be eliminated nonlinearly:
\begin{equation}
    \begin{aligned}
        \uu^{n+1} & =                       \\
                  & \uu^{n} + \inc t\biggl[
        b_1 \R^n                            \\
                  & +
            b_2 \R(t^{n+c_2},a^n_0\uu^{n - 1} +
            a^n_1\uu^{n} +
            a^n_2\uu^{n + 1}
            +
            \inc t^n
            \left(
            d^n_1\R^{n} +
            d^n_2\R^{n + 1}
        \right))                            \\
                  & +
            b_3 \R(t^{n+1},\uu^{n+1})
            \biggr]
        \label{eq:nestedDITR}
    \end{aligned}
\end{equation}
which we rewrite into the general form required in section \ref{sssec:dualTime}:
\begin{equation}
    \begin{aligned}
        \FF(\uu^{n+1}) & = -\frac{\uu^{n+1} - \uu^{n}}{\inc t^n} \\
                       & +
        b_1 \R^n                                                 \\
                       & +
        b_2 \R\left(t^{n+c_2},a^n_0\uu^{n - 1} +
        a^n_1\uu^{n} +
        a^n_2\uu^{n + 1}
        +
        \inc t^n
        \left(
        d^n_1\R^{n} +
        d^n_2\R^{n + 1}
        \right)\right)                                           \\
                       & +
        b_3 \R(t^{n+1},\uu^{n+1}) = 0
        \label{eq:nestedDITR_F}
    \end{aligned}
\end{equation}
Next, using the linearized implicit backward Euler approximation,
namely equation \eqref{eq:abstractNewton}, DITR methods can be solved.

The huge advantage of this approach is that only one equation is solved,
with only the next step value as unknown.
This approach requires $\R$ to be nested in another $\R$ evaluation and
is referred to as the nested solving approach.

The nested solving approach exploits the
local explicitness of DITR's formulation.
The nested solving approach is similar, and sometimes equivalent with
the solving approaches used in MIRK \cite{cash1975classMIRKOrig,cash1977clasMIRK1,cash1982monoMIRK2}
and NIRK \cite{kulikov2006familyNIRKOrig,kulikov2009adaptive,kulikov2007asymptotic}.
However, previous research on MIRK and NIRK methods have not
applied such solving approaches to flow problems.

Numerical result indicate that with small $\inc\tau$
and relatively large $\inc t$,
the nested solving approach usually fails to converge even in
linear problems.
Since dealing with large local CFL number with local
pseudo time stepping is an essential ability of implicit
time stepping methods,
this divergent result denies the practicality of the
nested solving approach.

\newcommand{\imagUnit}{\mathrm{i}}

To explain the failure of the nested solving approach,
a simple analysis with the linear convection problem
solved on an infinitely large 1-D uniform
grid.
Suppose the 1-D linear convection problem reads
\begin{equation}
    \partialderivative{u}{t} = -a \partialderivative{u}{x}
\end{equation}
with positive convection speed $a$.
Some difference scheme on uniform grid is used as spacial discretization:
\begin{equation}
    \derivative{u_i}{t} = -\frac{a}{\inc x} \sum_{j\in S_i}u_j\alpha_j
    \label{eq:fd}
\end{equation}
where $\alpha_j$ is the coefficients for finite difference approximation
and $S_i$ is the stencil set.
As the problem is linear,
a simple wave solution
\begin{equation}
    u = A(t)\exp(\imagUnit \frac{\kappa}{\inc x} x)
\end{equation}
can be substituted in and produces
a relation of
\begin{equation}
    \frac{\dd A(t)}{\dd t} = -\imagUnit a\frac{\kappa'}{\inc x}  A(t)
\end{equation}
where $\kappa'$ is the modified non-dimensional wave number,
and $\imagUnit$ is the imaginary unit.
For a finite difference scheme in equation \eqref{eq:fd},
$\kappa'$ can be found as
\begin{equation}
    \kappa' = \frac{1}{\imagUnit} \sum_{j\in S_i}\exp(\imagUnit (j-i)\kappa)\alpha_j
\end{equation}
The modified wave number $\kappa'$ is commonly used to evaluate the performance of
spacial discretization schemes.
A stable discretization would require $\imag(\kappa) \leq 0$ so that
$A(t)$ does not exponentially increase in magnitude.

Next, the classic linear analysis on dissipation and dispersion is
extended to dual time stepping. The dual time
stepping equation \eqref{eq:exactDualTime} can be
considered an evolution of the solution.
Thus, for the linear convection problem,
a simple wave pseudo time solution
\begin{equation}
    u^*(\tau,x) = A^*(\tau)\exp(\imagUnit \frac{\kappa}{\inc x} x)
    \label{eq:simpleWaveTestTau}
\end{equation}
where $*$ represent a certain unknown stage or step value.
Suppose
\begin{equation}
    \frac{\dd A^*(\tau)}{\dd \tau} = -\imagUnit a\frac{\kappa^*}{\inc x}  A^*(\tau) + C^*
    \label{eq:kappaSDefTau}
\end{equation}
then $\kappa^*$ represents the modified wave number for pseudo time.
The $C$ term in the above equation is a constant coefficient arising
from the transient terms in the time stepping method.
In order to find $\kappa^*$, $\FF^*$ need to be specified.
If we assume $\FF^*$ is in the form of
\begin{equation}
    \FF^*(\uu^*) = \mathbf{C} - \frac{\mathcal{A}}{\inc t} \uu^* + \mathcal{B}\R(t^*, \uu^*)
    \label{eq:canonicalFF}
\end{equation}
where $\mathbf{C}$ is the part of vectors irrelevant with $\uu^*$,
$\mathcal{A},\mathcal{B}$ are positive coefficients.
Evidently ESDIRK, BDF and trapezoid rule (as in equation \eqref{eq:trapF})
can satisfy this form.
Using the finite difference \eqref{eq:fd}, $\R$ is determined, and it is
shown that for $\FF$ having equation \eqref{eq:canonicalFF}'s form,
$\kappa^*$ is found to be
\begin{equation}
    \kappa^* = -\imagUnit\frac{\mathcal{A}}{\CFLt} +
    \frac{\mathcal{B}}{\imagUnit} \sum_{j\in S_i}\exp(\imagUnit (j-i)\kappa)\alpha_j
    =-\imagUnit\frac{\mathcal{A}}{\CFLt} + \mathcal{B}\kappa'
\end{equation}
where $\CFLt = \frac{a\inc t }{\inc x}$ is the physical CFL number.
When the finite difference is stable, $\imag (\kappa') \leq 0$, and consequently
$\imag (\kappa^*) < 0$.

To summarize, for stage residual function $\FF^*$ in the form of
equation \eqref{eq:canonicalFF}, it is guaranteed $\imag (\kappa^*) < 0$,
and the pseudo time evolution is stable in the sense of a continuous $\tau$.

This result confirms the reason behind using dual time stepping,
that it is automatically stable with certain ODE methods.

Back to the current section's subject,
DITR's nested solving approach,
where $\FF$ is defined with equation \eqref{eq:nestedDITR_F},
does not satisfy the form of equation \eqref{eq:canonicalFF},
as a result of nested $\R$ terms.
Continuing with a general finite difference scheme will
complicate the analysis, so we choose the first-order
upwind difference as an example:
\begin{equation}
    \derivative{u_i}{t} = -a \frac{u_i - u_{i-1}}{\inc x}
    \label{eq:fdup1}
\end{equation}
Therefore from equations \eqref{eq:nestedDITR_F} and \eqref{eq:canonicalFF} we have:
\begin{equation}
    \begin{aligned}
        \frac{\dd u_i^{n+1}}{\dd \tau} & =
        -\frac{u_i^{n+1} - u_i^{n}}{\inc t^n}      \\
                                       & +
        b_1 (-a\frac{u_i^{n}-u_{i-1}^{n}}{\inc x}) \\
                                       & +
        b_2 \Bigl[-\frac{a}{\inc x} \nabla_i
        \bigl[
        a^n_0u^{n - 1}_i +
        a^n_1u^{n}_i +
        a^n_2u^{n + 1}_i                           \\
                                       & +
        \inc t^n
        \left(
        d^n_1(-a\frac{u_i^{n}-u_{i-1}^{n}}{\inc x}) +
        d^n_2(-a\frac{u_i^{n+1}-u_{i-1}^{n+1}}{\inc x})
        \right)\bigr]  \Bigr]                      \\
                                       & +
        b_3 (-a\frac{u_i^{n+1}-u_{i-1}^{n+1}}{\inc x})
        \label{eq:nestedDITR_F1}
    \end{aligned}
\end{equation}
where $\nabla_i(u_i) = u_i - u_{u_i-1}$ is the $i$ direction
backward difference operator.
Since only $n+1$ relevant part of
the right side is of interest (which is non-constant in
the time step solving), the equation \cite{eq:nestedDITR_F1} can be reformed into:
\begin{equation}
    \begin{aligned}
        \frac{\dd u_i^{n+1}}{\dd \tau}\frac{\inc x}{a} & =
        C_i^n                                              \\
                                                       &
        -  \frac{1}{\CFLt} u_i^{n+1}
        -  b_2a_2^n u_i^{n+1}
        +  b_2a_2^n u_{i-1}^{n+1}
        \\
                                                       &
        + \CFLt b_2 d_2^n u_i^{n+1}
        - \CFLt b_2 d_2^n u_{i-1}^{n+1}
        \\
                                                       &
        - \CFLt b_2 d_2^n u_{i-1}^{n+1}
        + \CFLt b_2 d_2^n u_{i-2}^{n+1}
        \\
                                                       &
        -b_3 u_i^{n+1}
        +b_3 u_{i-1}^{n+1}
        \\
                                                       &
        =C_i^n  +  \CFLt b_2 d_2^n u_{i-2}^{n+1}
        \\
                                                       & +
        (-2\CFLt b_2 d_2^n + b_2a_2^n + b_3)u_{i-1}^{n+1}
        \\
                                                       & +
        (\CFLt b_2 d_2^n - b_3 - b_2a_2^n -  \frac{1}{\CFLt})u_i^{n+1}
        \label{eq:nestedDITR_F2}
    \end{aligned}
\end{equation}
where $C_i^m$ are terms irrelevant with $u_i^{n+1}, u_{i-1}^{n+1}\dots$
and remain constant throughout the pseudo time evolution.

Once again simple wave solution from equation \eqref{eq:simpleWaveTestTau}
is applied in \eqref{eq:nestedDITR_F2}, and comparing with
\eqref{eq:kappaSDefTau}, it is found
\begin{equation}
    \begin{aligned}
        \kappa^* = & \imagUnit
        \Biggl[
            \CFLt b_2 d_2^n \exp(-2\imagUnit\kappa)
            +
            (-2\CFLt b_2 d_2^n + b_2a_2^n + b_3)\exp(-\imagUnit\kappa)
        \\ + &
            (\CFLt b_2 d_2^n - b_3 - b_2a_2^n -  \frac{1}{\CFLt})
            \Biggr]
    \end{aligned}
\end{equation}
and therefore the imaginary part:
\begin{equation}
    \begin{aligned}
        \imag(\kappa^*) = &
        \CFLt b_2 d_2^n \cos(2\kappa)
        +
        (-2\CFLt b_2 d_2^n + b_2a_2^n + b_3)\cos(\kappa)
        \\ + &
        (\CFLt b_2 d_2^n - b_3 - b_2a_2^n -  \frac{1}{\CFLt})
        \\
        =                 &
        \CFLt b_2 d_2^n \left[\cos(2\kappa) -2\cos(\kappa) + 1\right]
        + (b_2a_2^n + b_3) (\cos(\kappa) - 1) - \frac{1}{\CFLt}
    \end{aligned}
    \label{eq:imagKsTauDITR}
\end{equation}

In equation \eqref{eq:imagKsTauDITR},
the $\left[\cos(2\kappa) -2\cos(\kappa) + 1\right]$ term
is negative for $\kappa \in (0, \pi/2)$, and for
all current DITR methods, $b_2>0$ and $d_2^n < 0$.
There always exists a $\CFLt$ that is large enough
so that $\imag(\kappa^*(\kappa)) \leq 0, \forall \kappa$ is false.
Which means for a large enough physical CFL number, the nested
solving approach is unstable in the sense of continuous $\tau$
evolution. 

As an example, DITR U2R2 method with $c_2=1/2$ gives
a specific formulation of $\imag(\kappa^*)$ that only depends on $\kappa$
and $\CFLt$, which is illustrated in figure \ref{fig:HM3_U2R2050_KappaS}.
Figure \ref{fig:HM3_U2R2050_KappaS} indicates for DITR U2R2 $c_2=1/2$,
when using nested solving approach, it is unable to linearly converge
with small $\inc \tau$ and approximately $\CFLt > 8$. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{pics/HM3_U2R2050_KappaS.pdf}
    \caption[]{DITR U2R2 $c_2=1/2$ imaginary part of $\kappa^*$ using 
    nested solving}
    \label{fig:HM3_U2R2050_KappaS}
\end{figure}

According to numerical tests with linear convection problem,
when $\inc \tau \rightarrow \infty$, the nested solving 
approach is possibly convergent. However, for strongly nonlinear
flow problems, a small $\inc \tau$ is almost a must, making 
the nested solving approach inapplicable.


\subsection{discard}

HIRK's Butcher tableau in table \ref{tab:HM3Butcher}
indicates that apart from the first stage,
the other two stages seem fully coupled,
but following the original formulation
of HIRK \eqref{eq:HM3} which originates from
interpolation and integration,
clearly the internal stage $\uu^*$ can be explicitly
expressed by $\uu^n, \uu^{n+1}$ with the interpolation
relation \eqref{eq:HM3-2}, and one only need
to solve \eqref{eq:HM3-1} implicitly.
Therefore, the first solving strategy this paper
considers is identical to the spirit of NIRK,
which solves the equation:
\begin{equation}
    \label{eq:eqFull}
    \uu^{n+1} -
    \left(\uu^{n} + \inc t
    \left[
        b_1\R(t^{n,1}, \uu^n) +
        b_2\R(t^{n+c_2}, \uu^*) +
        b_3\R(t^{n,3}, \uu^{n+1})
        \right]
    \right)  =  0
\end{equation}
For simplicity, we denote the residual of \eqref{eq:eqFull}
as:
\begin{equation}
    \Res(\uu^{n+1}) \doteq \uu^{n+1} -
    \left(\uu^{n} + \inc t
    \left[
        b_1\R(t^{n,1}, \uu^{n}) +
        b_2\R(t^{n+c_2}, \uu^{*}) +
        b_3\R(t^{n,3}, \uu^{n+1})
        \right]
    \right)
\end{equation}
Considering $\uu^{n}$ is known,
and $\uu^{*}$ is explicitly expressed,
$\Res(\uu^{n+1})$ is considered only a
function of unknown $\uu^{n+1}$.
The Newton-Raphson method applied to
\eqref{eq:eqFull}'s solving would be
\begin{equation}
    \begin{aligned}
        \uu^{n+1,m+1}                  & =  \uu^{n+1,m} + \inc \uu^{n+1,m}, \\
        \frac{\partial \Res(\uu^{n+1,m})}
        {\uu^{n+1,m}} \inc \uu^{n+1,m} & = - \Res(\uu^{n+1,m})
    \end{aligned}
\end{equation}
and adding the pseudo time continuation
would be
\begin{equation}
    \label{eq:ntDtau}
    \begin{aligned}
        \uu^{n+1,m+1}            & =  \uu^{n+1,m} + \inc \uu^{n+1,m}, \\
        \left(
        \frac{\partial \Res(\uu^{n+1,m})}
        {\uu^{n+1,m}} + \inc t \Tau^{-1}
        \right) \inc \uu^{n+1,m} & = - \Res(\uu^{n+1,m})
    \end{aligned}
\end{equation}
where $\Tau=\mathrm{diag}(\tau_1,...\tau_N)$ is a diagonal matrix with
local pseudo time steps as its diagonal values.
The pseudo time continuation of Newton iteration
could be both understood as a relaxation of original
method for nonlinear stability,
or as the result of solving dual time stepping
method Jameson developed \cite{jameson1991time}.
In other words, \eqref{eq:ntDtau} is the result
of solving pseudo time evolution
$\frac{d\uu^{n+1}}{d\tau} = -\frac{\Res}{\inc t}$
with linearly implicit backward Euler method and
a local time stepping.
Such dual time stepping is prevalent in transient
flow solution due to the nonlinearity in
fluid systems.
However, pseudo time term could
cause trouble if we try to solve HIRK
of any MIRK type system with certain approaches, which
will be illustrated later.

The expanded form of residual's Jacobian
is
\begin{equation}
    \label{eq:fullJCB}
    \begin{aligned}
        \frac{\partial \Res(\uu^{n+1})}
        {\uu^{n+1}}
                     & = \eye - \inc t b_3 \J^{n+1} -
        \inc t b_2\J^*(a_2\eye +\inc t d_2 \J^{n+1})                                  \\
                     & = \frac{d_2 + a_2b_3}{d_2}\eye
        -\frac{a_2b_3}{d_2}\left(
        \eye + \frac{d_2b_2}{b_3}\inc t\J^*
        \right)\left(
        \eye + \frac{d_2}{a_2}\inc t\J^{n+1}
        \right)                                                                       \\
        \text{with:} &                                                                \\
                     & \J^* = \frac{\partial \R(t^{n+c_2},\uu^*)}{\partial \uu^*},\ \
        \J^{n+1} = \frac{\partial \R(t^{n,3},\uu^{n+1})}{\partial \uu^{n+1}}
    \end{aligned}
\end{equation}
which is a quadratic polynomial of right-hand-sides'
Jacobian.
If the Jacobian matrices could be explicitly formed and
manipulated, solving \eqref{eq:ntDtau} would be straightforward.
But a lot of high order methods rely on matrix-free
Jacobian evaluation, thus solving the linear system
with its matrix in the form of \eqref{eq:fullJCB}
would be troublesome or impossible.
Even if one would like to use a matrix-less linear solver
like Krylov space solvers,
powerful matrix-free preconditioners like ILU(0) or
LUSGS would be unavailable.

The solution to the complexity of Jacobian
by Kulikov and Shindin
is using an approximate factorization
\cite{kulikov2006familyNIRKOrig,kulikov2009adaptive}.
For the forth order
Gauss type NIRK in \cite{kulikov2006familyNIRKOrig},
Kulikov and Shindin used $(\eye-\inc t /4 \J)^2$
as an approximation.
This paper considers the form
\begin{equation}
    \label{eq:factJCB}
    \begin{aligned}
        \frac{\partial \Res(\uu^{n+1})}
        {\uu^{n+1}}
         & \approx
        -\frac{a_2b_3}{d_2}\left(
        \eye + \frac{d_2b_2}{b_3}\inc t\J^*
        \right)\left(
        \eye + \frac{d_2}{a_2}\inc t\J^{n+1}
        \right)    \\
    \end{aligned}
\end{equation}
which is directly taken from the last term
in \eqref{eq:fullJCB}, and the omitted term
is identity matrix with coefficient $\frac{d_2 + a_2b_3}{d_2}>0$
for $c_2\in(0,1)$. Note that $d_2<0$ for any possible $c_2$.

Using a factorized form of approximate
Jacobian like \eqref{eq:factJCB} enables the use of
two successive matrix-free solvers to obtain
the increment for Newton iteration.

For practical CFD implementation,
pseudo time term is now considered.
In order to preserve the factorized form,
this paper finds using
\begin{equation}
    \label{eq:ntDtau_Fact}
    \begin{aligned}
        \uu^{n+1,m+1}    & =  \uu^{n+1,m} + \inc \uu^{n+1,m}, \\
        -\frac{a_2b_3}{d_2}\left(
        \eye + \frac{d_2b_2}{b_3}\inc t\J^*
        + \inc t \Tau^{-1}
        \right)\left(
        \eye + \frac{d_2}{a_2}\inc t\J^{n+1}
        \right)
        \inc \uu^{n+1,m} & = - \Res(\uu^{n+1,m})
    \end{aligned}
\end{equation}
as the pseudo time continued Newton iteration
satisfactory.
In the linear sense, using this this approximate
Jacobian and pseudo time stepping does not sabotage
the convergence of Newton iteration, which can
be shown by applying the iteration \eqref{eq:ntDtau_Fact}
to solving scalar ODE $\frac{dy}{dt} = \lambda y$.
For multi-dimensional linear ODEs, using a Jordan decomposition
would make the proof reduce to scalar equation. \#\#\#\#


% Due to the explicit nature of the internal 
% stage $\uu^*$, when solving nonlinear systems,
% the un-converged state of $\uu^{n+1}$ at 
% the beginning of iteration could put $\uu^*$
% far from the real solution, 
% causing the right-hand-side to behave bad
% and possibly make the iteration breakdown. 
% Therefore, this paper presents another approach of solving 
% HIRK step which is vital for nonlinear ODEs.

% In order to account for nonlinearity, 
% we forsake the explicit nature in the 
% determination of $\uu^*$ as described in \eqref{eq:HM3-2}, 
% which is an Hermite interpolation linear dependent 
% on $\uu^{n},\uu^{n+1},\R^n, \R^{n+1}$.

This paper also provides another kind of
solving strategy,
which forsakes the explicitness in \eqref{eq:HM3-2},
thus making $\uu^*$ the dof to be solved,
which is the case for a general IRK scheme.
The equation to solve would be:
\begin{equation}
    \begin{aligned}
         & \left\{
        \begin{array}[2]{ll}
            \Res(\uu^{n+1},\uu^{*})   & = 0 \\
            \Res^*(\uu^{n+1},\uu^{*}) & = 0 \\
        \end{array}
        \right.    \\
    \end{aligned}
\end{equation}
with
\begin{equation}
    \begin{aligned}
        \Res(\uu^{n+1},\uu^{*})   & \doteq \uu^{n+1} - \left(
        \uu^{n} +
        \inc t
        \left[
            b_1\R(t^{n,1}, \uu^n) +
            b_2\R(t^{n+c_2}, \uu^*) +
            b_3\R(t^{n,3}, \uu^{n+1})
            \right]
        \right)                                               \\
        \Res^*(\uu^{n+1},\uu^{*}) & \doteq\uu^{*} - \left(
        a_1\uu^{n} +
        a_2\uu^{n+1} +
        \inc t
        \left[
            d_1\R(t^{n,1}, \uu^n) +
            d_2\R(t^{n,3}, \uu^{n+1})
            \right]
        \right)
    \end{aligned}
\end{equation}
Making $\uu^*$ unknown has two major drawbacks.
The first drawback is that additional memory
would be required for $\uu^*$'s storage.
Considering HIRK saves much space compared to
DIRK methods, and HIRK also requires some more space
for intermediate results even using explicit $\uu^*$ evaluation,
the first drawback would not be significant.
The second drawback is the increase in the dimension of
algebraic system.
Normally, the enlarged system can be solved
using appropriate blocking strategy,
for example in \cite{pazner2017stage}.
However, the current scheme takes a different form of
iteration which takes two sub-steps:
\begin{equation}
    \label{eq:intersolve}
    \begin{aligned}
        \uu^{*,m+1}   & = \uu^{*,m} - \left[
            \eye - \inc t \frac{\partial \Res^{*'}(\uu^{n+1,m},\uu^{*,m})}
            {\partial \uu^{*,m}}
            + \inc t \Tau^{-1}
            \right]
        \backslash \Res^{*'}(\uu^{n+1,m},\uu^{*,m}) \\
        \uu^{n+1,m+1} & = \uu^{n+1,m} - \left[
            \eye - \inc t \frac{\partial \Res^{}(\uu^{n+1,m},\uu^{*,m+1})}
            {\partial \uu^{n+1,m}}
            + \inc t \Tau^{-1}
            \right]
        \backslash \Res^{}(\uu^{n+1,m},\uu^{*,m+1}) \\
    \end{aligned}
\end{equation}
where the $\mathbf{A}\backslash \mathbf{b}$
notation means to solve the linear problem $\mathbf{A}\mathbf{x}=\mathbf{b}$ once.
The residual $\Res^{*'}$ is defined with:
\begin{equation}
    \Res^{*'}(\uu^{n+1},\uu^{*}) = \Res^*(\uu^{n+1},\uu^{*}) + \beta\Res(\uu^{n+1},\uu^{*})
\end{equation}
which serves as the residual preconditioned with transformation
\begin{equation*}
    \begin{bmatrix}
        \Res^{} \\
        \Res^{*'}
    \end{bmatrix} =
    \begin{bmatrix}
        \eye & 0 \\ \beta\eye & \eye
    \end{bmatrix}
    \begin{bmatrix}
        \Res^{} \\
        \Res^{*}
    \end{bmatrix}
\end{equation*}
with $\beta\in\mathbb{R}$.
In \cite{jameson2017evaluation} Jameson used the inverse of Butcher coefficients
for preconditioning, which makes off-diagonal blocks of
the Jacobian scalar.
That preconditioning could
be adopted in the current method,
but is not recommended for
possible instabilities in the presence of negative pressure fixing in
numeric tests. \#\#\#\#

The solving procedure \eqref{eq:intersolve} could be regarded as
successively updating $\uu^*$ with $\Res^{*'}$ and $\uu$
with $\Res^{}$, by using the latest values of the other DOF.
Within a linear context,
this would be a blocks Gauss-Seidel iteration
over the $2\times2$ blocks of system with each block being $N\times N$.
In other words,to solve:
\begin{equation*}
    \begin{bmatrix}
        \Res^{}(\uu^{n+1},\uu^{*}) \\
        \Res^{*'}(\uu^{n+1},\uu^{*})
    \end{bmatrix} = 0
\end{equation*}
the canonical quasi Newton iteration would need to solve
\begin{equation}
    \label{eq:fullCoupled}
    \begin{bmatrix}
        \frac{\partial\Res^{}(\uu^{n+1,m},\uu^{*,m})}{\partial\uu^{n+1,m}} + \inc t\Tau^{-1} &
        \frac{\partial\Res^{}(\uu^{n+1,m},\uu^{*,m})}{\partial\uu^{*,m}}                       \\
        \frac{\partial\Res^{*'}(\uu^{n+1,m},\uu^{*,m})}{\partial\uu^{n+1,m}}                 &
        \frac{\partial\Res^{*'}(\uu^{n+1,m},\uu^{*,m})}{\partial\uu^{*,m}} + \inc t\Tau^{-1}   \\
    \end{bmatrix}
    \begin{bmatrix}
        \inc\uu^{n+1,m} \\
        \inc\uu^{*,m}
    \end{bmatrix}
    =
    -\begin{bmatrix}
        \Res^{}(\uu^{n+1,m},\uu^{*,m}) \\
        \Res^{*'}(\uu^{n+1,m},\uu^{*,m})
    \end{bmatrix}
\end{equation}
to update both stage values concurrently.
The current method \eqref{eq:intersolve}
first solve the second block row of
\eqref{eq:fullCoupled} for the increment of $\uu^*$,
then solve the first block row
for the increment of $\uu^{n+1}$ using updated $\uu^*$.
The current solving strategy will
be referred to as successive solving in the following parts.

The omitting of off-diagonal relations in \eqref{eq:fullCoupled},
or the block nonlinear Gauss-Seidel handling on the top layer of
algorithm brings about extra error in iteration,
causing slow-down of iteration and possible divergence.
To analyze such behavior, the current research investigates
the impact of successive solving \eqref{eq:intersolve}
brought to solution of HIRK applied to linear problems
using precise linear solvers.
With a traditional quasi Newton iteration,
if pseudo time steps approaches infinity,
the iteration would converge in the first step in such
situation.
The introduction of successive solving \eqref{eq:intersolve}
renders this one-step-convergence impossible, thus
the convergence with successive solving will represent
all the deceleration effect caused by it.
The linear situation discussed could also be a linearization
of a non-linear ODE when the stage values converge near the
solution, and the instant convergence of Newton iteration would
be first order accurate.
In the linear or linearized scene,
the residual takes the form:
\begin{equation}
    \label{eq:linearized}
    \begin{aligned}
        \Res^{}(\uu^{n+1},\uu^{*})   & =
        \Res^{}_0 - \inc t b_2\J^*\uu^{*} + (\eye - \inc t b_3\J)\uu^{n+1} \\
        \Res^{*'}(\uu^{n+1},\uu^{*}) & =
        \Res^{*'}_0 + (\eye - \beta\inc t b_2\J^*)\uu^{*}
        + ((\beta-a_2)\eye - \inc t (\beta b_3 - d_2)\J)\uu^{n+1}
    \end{aligned}
\end{equation}
where $\J^*, \J$ are Jacobian of $\R$
at the solution of
$\uu^{*}, \uu^{n+1}$.
Applying \eqref{eq:intersolve} to
this linear form and assuming a infinite pseudo
time step,
following textbook analysis conducted on
Gauss-Seidel iteration,
the successive solving becomes a linear
fixed point iteration, with fixed point
mapping's matrix being
\begin{equation}
    \mathbf{B}_{SS}=\begin{bmatrix}
        \eye - \beta\inc t b_2\J^* & 0                   \\
        - \inc t b_2\J^*           & \eye - \inc t b_3\J \\
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        0 & (\beta-a_2)\eye - \inc t (\beta b_3 - d_2)\J \\
        0 & 0                                            \\
    \end{bmatrix}
\end{equation}
Thus, convergence of the fixed point iteration equals to
the condition
\begin{equation}
    \label{eq:rhoGS}
    \begin{aligned}
         & \varUpsilon(\mathbf{B}_{SS}) \\
         & =\varUpsilon\left(
        -[\eye - \inc t b_3\J]^{-1}
        [- \inc t b_2\J^*]
        [\eye - \beta\inc t b_2\J^*]^{-1}
        [(\beta-a_2)\eye - \inc t (\beta b_3 - d_2)\J]
        \right)                         \\
         & < 1
    \end{aligned}
\end{equation}
where $\varUpsilon$ is spectral radius of matrix.
For arbitrary $\J,\J^*$, further analysis is difficult,
so we limit the situation to where $\J = \J^*$ is an acceptable
approximation, or the ODE is linear itself.
Using the Jordan form,
and assuming all the inverse are existent and precisely solved,
\eqref{eq:rhoGS} would become
\begin{equation}
    \label{eq:rhoGS1}
    \begin{aligned}
        \varrho_i =
        \left|
        -[1 -  b_3z_i]^{-1}
        [- b_2z_i]
        [1 - \beta b_2z_i]^{-1}
        [(\beta-a_2) - (\beta b_3 - d_2)z_i]
        \right| & < 1,\ \ \forall z_i = \inc t \lambda_i
    \end{aligned}
\end{equation}
where $\lambda_i$ is eigenvalues of $\J$.
By further assuming $\mathfrak{Re} z_i < 0$
which is true for stable semi-discretized
method used on bounded linear problems,
special cases were analyzed.
For $c_2=0.5, \beta=0.5$, it is found
$\varrho_i < 0.5$.
For $c_2=0.5, \beta=1$, it is found $\varrho_i < 0.42$.
For $c_2=0.55, \beta=0.5$, $\varrho_i$ could exceed 1.
For $c_2=0.55, \beta=1$, it is found $\varrho_i < 0.36$.
These analyses are trivial which involve finding maximal points
in the left complex plane and on the imaginary axis.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=3in]{pics/numSearchRho.png}
    \caption{Numerical search result }
    \label{fig:numSearchRho}
\end{figure}

Numerical search is conducted to find
\begin{equation}
    \varrho_m(\beta) = \max_{\forall i}(\varrho_i)
\end{equation}
whose result is shown in figure \ref{fig:numSearchRho}.
Figure \ref{fig:numSearchRho} indicates
at least for $c_2\in[0.5,0.6]$, $\varrho_m<1$
could be satisfied easily by adjusting $\beta$, therefore
achieving convergence in Gauss-Seidel iteration.
For optimal Gauss-Seidel convergence rate,
values of $\beta$ could be found by numeric searching
as shown in figure \ref{fig:numSearchRho}.
Figure \ref{fig:numSearchRho} also suggests
$c_2=0.5$ helps acquiring better convergence rage
in successive solving.

The current successive solving approach
differs from previous ones,
which only requires linear solutions of
the same dimension as the ODE.
This property makes HIRK's implementation
very similar to traditional DIRK or BDF methods,
thus taking less effort.





\section{Numerical Tests}

During numerical tests,
BDF2 and ESDIRK4 methods taken from
\cite{bijl2002implicitBDFvESDIRK,kennedy2003additiveARK}
are chosen to be
baseline time marching methods.
For HIRK method, instances of
$c_2 = 0.5$ and $c_2 = 0.55$ are tested.
For $c_2 = 0.5$, the method is 4th order accurate and
is essentially the Lobatto IIIA method \cite{wanner1996solving}.
For $c_2 = 0.55$ which has 3rd order accuracy,
the symmetry is broken and linear stability is
improved as clarified in \ref{ssec:linStab}.
Both HIRK methods employ successive
solving in the form of \eqref{eq:intersolve},
with $\beta$ chosen as empirical values $1,4/3$ respectively.


The isentropic vortex, two dimensional vortex shedding
and  double mach reflection problems use
$P^3$ variational reconstruction finite volume method declared in
section \ref{sec:CFV} as spacial discretization.
Iterative solution of the implicit reconstruction
is conducted before each right-hand-side evaluation,
which consists of 1 block-Jacobi iteration by default.
Pseudo time continued Newton iteration on mean values are
solved using 5 times of block-Jacobi iteration by default,
which is found both stable and efficient enough for VFV solving
transient problems.
It should be noted that, in other words,
both the reconstruction system and
the time stepping implicit system use linear solvers that
runs for a fixed number of steps without monitoring convergence.
Only the magnitude of the nonlinear time stepping residual
is monitored to ensure convergence.
This linear solving method simplifies the control flow of
the program,
while providing a simple yet reliable way of
comparing total calculation consumption between ODE solvers,
which will be explained in section \ref{ssec:resultIV}.



\subsection{Isentropic Vortex}
\label{ssec:resultIV}

The isentropic vortex problem is a classic
accuracy testing problem for Euler equations.
The settings can be found in \cite{hu1999weighted_WENO}.
The free-stream flow is $(\rho,u,v,p)=(1,1,1,1)$,
and a perturbation at initial time:
\begin{equation}
    \left\{
    \begin{array}[2]{ll}
        (\delta u, \delta v) & = \frac{\epsilon}{2\pi}\exp(\frac{1-[(x-x_c)^2+(y-y_c)^2]}{2})(-y+y_c,x-x_c) \\
        \delta T             & = - \frac{(\gamma-1)\epsilon^2}{8\gamma\pi^2}\exp(1-[(x-x_c)^2+(y-y_c)^2])   \\
        \delta S             & = 0                                                                          \\
    \end{array}
    \right.
\end{equation}
with ideal gas setting of $T = p/\rho, S= p/\rho^\gamma, \gamma =1.4$.
Initial vortex center is chosen $(x_c,y_c)=(5,5)$,
and vortex strength is $\epsilon = 5$.
The analytic solution to the isentropic vortex problem is a
translation of initial field with speed $(1,1)$.
The computational domain is $[0,10]\times[0,10]$,
using periodic boundary conditions.

First, the implicit ODE integrators are tested with large
time steps.
The mesh is $40\times40$ square grid,
and solution is calculated until $t=10$ with $\inc t=1$.
The CFL number based on $\inc t, \inc x$ is roughly $11$,
making the propagation of the vortex hard to
simulate.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/IV40_10steps_HM3LBT.png}
        \caption[]{HIRK $c_2=0.5$}
        \label{sfig:IV10Step_HM3LBT}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/IV40_10steps_HM3.png}
        \caption[]{HIRK $c_2=0.55$}
        \label{sfig:IV10Step_HM3}
    \end{subfigure}\\
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/IV40_10steps_ESDIRK4.png}
        \caption[]{ESDIRK4}
        \label{sfig:IV10Step_ESDIRK4}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/IV40_10steps_BDF2.png}
        \caption[]{BDF2}
        \label{sfig:IV10Step_BDF2}
    \end{subfigure}
    \caption{Density distribution of isentropic vortex problem, with $\inc t = 1$ at $t=10$}
    \label{fig:IV10Step}
\end{figure}

Results of large time step testing are shown in figure \ref{fig:IV10Step}.
Clearly, from figure \ref{sfig:IV10Step_BDF2}, BDF2 almost completely
smears the initial vortex with only 10 steps for one period.
The higher order methods somehow preserves the characteristics of
a vortex. HIRK $c_2=0.5$ produces significant numerical oscillation along
the propagation direction as shown in figure \ref{sfig:IV10Step_HM3LBT},
while the $c_2=0.55$ HIRK inhibits them better in figure \ref{sfig:IV10Step_HM3}.
The peak value in the vortex center produced by HIRK $c_2=0.5$ is comparable with
ESDIRK4, while HIRK $c_2=0.55$ gives a flatter density peak.

Next, precision and efficiency of different ODE methods are
qualitatively evaluated with isentropic vortex solved on a $160\times160$
grid until $t=2$.
The fine mesh makes spacial error negligible compared with
temporal error.
The density error is defined as
\begin{equation}
    \epsilon_\rho = \frac{\int_{(x,y)\in[4,9]\times[4,9]}{|\rho-\rho_a dxdy}|}{
    |[4,9]\times[4,9]|
    }
\end{equation}
with $[4,9]\times[4,9]=25$ the area of evaluating region, $\rho$ the numeric result
and $\rho_a$ the analytic result.
The evaluating region is limited to $[4,9]\times[4,9]=25$
where the vortex center
and most numerical error resides.



\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/HM3_IV160_fig_1.pdf}
        \caption[]{Density error vs. time step size }
        \label{sfig:IVTests_Conv}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/HM3_IV160_fig_2.pdf}
        \caption[]{Density error vs. total CPU time consumption}
        \label{sfig:IVTests_Eff}
    \end{subfigure}
    \caption[]{Convergence and efficiency test with isentropic vortex problem}
    \label{fig:IVTests}
\end{figure}

Results of convergence and efficiency study with isentropic vortex is
shown in figure \ref{fig:IVTests}.
Figure \ref{sfig:IVTests_Conv} indicates that all the ODE methods
could roughly achieve their theoretical order of accuracy.
With the same time step size, figure \ref{sfig:IVTests_Conv} shows
ESDIRK4 has best accuracy, while HIRK $c_2=0.5$ is close to
ESDIRK4 when time step is refined. HIRK $c_2=0.55$ is less accurate
than HIRK $c_2=0.5$, but it performs almost as well as
HIRK $c_2=0.5$ when time step is large.
Figure \ref{sfig:IVTests_Eff}
implies that when consuming the same computational resource,
HIRK methods have best accuracy or efficiency. All the high order
time marching methods have better efficiency than BDF2,
while HIRK methods have better efficiency than ESDIRK4.
The symmetric HIRK $c_2=0.5$ has better efficiency
compared with more stable HIRK $c_2=0.55$.

The convergence and efficiency study with isentropic vortex was
conducted on a isolated server with 2 Intel Xeon 6326 processors with
16 cores each, with measures to ensure the stability of CPU time measurement.
Further tests with vortex shedding and double mach reflection
were conducted on a large cluster, which unfortunately produces more
unstable CPU time due to disturbances of the communication network.
Therefore, following numerical tests will use the effective iterations
to represent total computational consumption.
Due to the settings of the current research, each updating in
the implicit solving consumes almost identical time.
For BDF2, each right-hand-side evaluation, linear solving and updating
is counted as $1.0$ effective iteration.
For ESDIRK4, as each updating of some stage value is quite similar with
BDF2 apart from some extra linear summations, each updating is also
counted as $1.0$ effective consumption.
For HIRK methods, each updating of $\uu^*,\uu^{n+1}$
involves two right-hand-side evaluations and linear solver executions.
Therefore, each HIRK updating is counted as $2.0$ effective iterations.
Although one could calibrate the coefficients using theoretical analyses or
empirical data to get a more precise measurement of consumption,
the current simple coefficients for effective iterations are found to
be sufficient.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/HM3_IV160_fig_21.pdf}
        \caption[]{Density error vs. effective iterations }
        \label{sfig:IVTests2_EffIt}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/HM3_IV160_fig_4.pdf}
        \caption[]{CPU time vs. effective iterations}
        \label{sfig:IVTests2_CPUVEffIt}
    \end{subfigure}
    \caption[]{The impact of using iteration counting to measure total work}
    \label{fig:IVTests2}
\end{figure}

Figure \ref{fig:IVTests2} illustrates the impact of using effective iteration
as total computational work. Qualitatively, figure \ref{sfig:IVTests2_EffIt}
is identical to figure \ref{sfig:IVTests_Eff},
and figure  \ref{sfig:IVTests2_CPUVEffIt} shows strong linear correlation
between CPU time and effective iterations with their
$log_{10}$ values having correlation coefficient $r=0.9943$.
Therefore, the cases of isentropic vortex which are conducted in
a stable environment indicate that the effective iteration is
a reliable measurement of computational consumption.

\subsection{Two Dimensional Vortex Shedding}

Vortex shedding from a circular cylinder and forming a vortex street
is a classic test problem for transient fluid simulation. Due to the refined
mesh near solid wall, such cases usually prefer implicit time marching
over explicit ones whose time steps are bounded by CFL condition.
The current paper studies the 2D laminar case, where
Reynolds number
$Re_d=\rho_\infty u_\infty d / \mu_\infty $ is around $10^3$,
with Mach number $Ma=0.1$ which makes the flow almost incompressible.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{pics/CylinderA1_Re2000_Mesh.png}
    \caption[]{Part of mesh used in $Re=2000$ vortex shedding problem}
    \label{fig:CylinderRe2000_Mesh}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/CylinderA1_Re2000_HM3LBT.png}
        \caption[]{HIRK $c_2=0.5$}
        \label{sfig:CylinderRe2000_HM3LBT}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/CylinderA1_Re2000_HM3.png}
        \caption[]{HIRK $c_2=0.55$}
        \label{sfig:CylinderRe2000_HM3}
    \end{subfigure}
    \caption[]{Comparison of pressure distribution in $Re=2000$ vortex shedding problem}
    \label{fig:CylinderRe2000}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/CylinderB1_Re1200_Mesh.png}
        \caption[]{Mesh}
        \label{sfig:CylinderRe1200Demo_Mesh}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/CylinderB1_Re1200.png}
        \caption[]{Vorticity distribution}
        \label{sfig:CylinderRe1200Demo_Vort}
    \end{subfigure}
    \caption[]{Mesh and a instance of z-vorticity distribution
        in $Re=1200$ vortex shedding problem}
    \label{fig:CylinderRe1200Demo}
\end{figure}



\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig14.pdf}
        \caption[]{Error vs. time step size}
        \label{sfig:CylinderRe1200_My_C}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig4.pdf}
        \caption[]{Error vs. effective iterations}
        \label{sfig:CylinderRe1200_My_E}
    \end{subfigure}
    \caption[]{Convergence and efficiency analysis with $\epsilon_{My}$ in $Re=1200$ vortex shedding problem}
    \label{fig:CylinderRe1200_My}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig15.pdf}
        \caption[]{Error vs. time step size}
        \label{sfig:CylinderRe1200_Mx_C}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig5.pdf}
        \caption[]{Error vs. effective iterations}
        \label{sfig:CylinderRe1200_Mx_E}
    \end{subfigure}
    \caption[]{Convergence and efficiency analysis with $\epsilon_{Mx}$ in $Re=1200$ vortex shedding problem}
    \label{fig:CylinderRe1200_Mx}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig13.pdf}
        \caption[]{Error vs. time step size}
        \label{sfig:CylinderRe1200_St_C}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/Cylinder_fig3.pdf}
        \caption[]{Error vs. effective iterations}
        \label{sfig:CylinderRe1200_St_E}
    \end{subfigure}
    \caption[]{Convergence and efficiency analysis with $\epsilon_{St}$ in $Re=1200$ vortex shedding problem}
    \label{fig:CylinderRe1200_St}
\end{figure}




\subsection{Double Mach Reflection}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/DM480_HM3LBT.png}
        \caption[]{HIRK $c_2=0.5$}
        \label{sfig:DM480_HM3LBT}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/DM480_HM3.png}
        \caption[]{HIRK $c_2=0.55$}
        \label{sfig:DM480_HM3}
    \end{subfigure}\\
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/DM480_ESDIRK.png}
        \caption[]{ESDIRK4}
        \label{sfig:DM480_ESDIRK4}
    \end{subfigure}\hfill
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{pics/DM480_BDF.png}
        \caption[]{BDF2}
        \label{sfig:DM480_BDF2}
    \end{subfigure}
    \caption{Density distribution in double mach reflection problem}
    \label{fig:DM480}
\end{figure}

\section{Conclusion}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num}
\bibliography{HM3DraftRefs.bib}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

%     %% \bibitem{label}
%     %% Text of bibliographic item

%     \bibitem{}

% \end{thebibliography}


\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
